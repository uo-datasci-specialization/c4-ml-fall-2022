{
  "articles": [
    {
      "path": "assignment1.html",
      "title": "Assignment 1",
      "author": [
        {
          "name": "Cengiz Zopluoglu",
          "url": {}
        }
      ],
      "date": "12-08-2020",
      "contents": "\r\n\r\nContents\r\nR Markdown\r\nIncluding Plots\r\n\r\nR Markdown\r\nThis is an R Markdown document. Markdown is a simple formatting\r\nsyntax for authoring HTML, PDF, and MS Word documents. For more details\r\non using R Markdown see http://rmarkdown.rstudio.com.\r\nWhen you click the Knit button a document will be\r\ngenerated that includes both content as well as the output of any\r\nembedded R code chunks within the document. You can embed an R code\r\nchunk like this:\r\n\r\n\r\nsummary(cars)\r\n\r\n     speed           dist       \r\n Min.   : 4.0   Min.   :  2.00  \r\n 1st Qu.:12.0   1st Qu.: 26.00  \r\n Median :15.0   Median : 36.00  \r\n Mean   :15.4   Mean   : 42.98  \r\n 3rd Qu.:19.0   3rd Qu.: 56.00  \r\n Max.   :25.0   Max.   :120.00  \r\n\r\nIncluding Plots\r\nYou can also embed plots, for example:\r\n\r\n\r\n\r\nNote that the echo = FALSE parameter was added to the\r\ncode chunk to prevent printing of the R code that generated the\r\nplot.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-07-06T19:20:54-07:00"
    },
    {
      "path": "index.html",
      "title": "Course Title",
      "description": "Short Course description\n",
      "author": [],
      "contents": "\r\nLong course description\r\n\r\n\r\n\r\n",
      "last_modified": "2022-07-06T19:20:56-07:00"
    },
    {
      "path": "lecture-1.html",
      "title": "Introduction to Toy Datasets",
      "author": [
        {
          "name": "Cengiz Zopluoglu",
          "url": {}
        }
      ],
      "date": "06/23/2022",
      "contents": "\r\n\r\nContents\r\nReadability\r\nRecidivism\r\nInstalling\r\nReticulate, Miniconda, and Sentence Transformers\r\n\r\n\r\n\r\n[Updated: Wed, Jul 06, 2022 - 19:20:59 ]\r\nThere are two datasets we will use throughout this course. The first\r\ndataset has a continuous outcome and the second dataset has a binary\r\noutcome. We will apply several methods and algorithms to these two\r\ndatasets during the course. We will have an opportunity to compare and\r\ncontrast the prediction outcomes from several models and methods on the\r\nsame datasets.\r\nThis section provides some background information and context for\r\nthese two datasets.\r\nReadability\r\nThe readability dataset comes from a recent Kaggle\r\nCompetition (CommonLit Readability Prize). You can directly download\r\nthe training dataset from the competition website, or you can import it\r\nfrom the course website.\r\n\r\n\r\nreadability <- read.csv(here('data/readability.csv'),header=TRUE)\r\n\r\nstr(readability)\r\n\r\n'data.frame':   2834 obs. of  6 variables:\r\n $ id            : chr  \"c12129c31\" \"85aa80a4c\" \"b69ac6792\" \"dd1000b26\" ...\r\n $ url_legal     : chr  \"\" \"\" \"\" \"\" ...\r\n $ license       : chr  \"\" \"\" \"\" \"\" ...\r\n $ excerpt       : chr  \"When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an inte\"| __truncated__ \"All through dinner time, Mrs. Fayre was somewhat silent, her eyes resting on Dolly with a wistful, uncertain ex\"| __truncated__ \"As Roger had predicted, the snow departed as quickly as it came, and two days after their sleigh ride there was\"| __truncated__ \"And outside before the palace a great garden was walled round, filled full of stately fruit-trees, gray olives \"| __truncated__ ...\r\n $ target        : num  -0.34 -0.315 -0.58 -1.054 0.247 ...\r\n $ standard_error: num  0.464 0.481 0.477 0.45 0.511 ...\r\n\r\nThere is a total of 2834 observations. Each observation represents a\r\nreading passage. The most significant variables are the\r\nexcerpt and target columns. The excerpt column\r\nincludes plain text data, and the target column includes a corresponding\r\nmeasure of readability for each excerpt.\r\n\r\n\r\nreadability[1,]$excerpt\r\n\r\n[1] \"When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.\\nThe floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.\\nAt each end of the room, on the wall, hung a beautiful bear-skin rug.\\nThese rugs were for prizes, one for the girls and one for the boys. And this was the game.\\nThe girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.\\nThis would have been an easy matter, but each traveller was obliged to wear snowshoes.\"\r\n\r\nreadability[1,]$target\r\n\r\n[1] -0.3402591\r\n\r\nAccording\r\nto the data owner, ‘the target value is the result of a\r\nBradley-Terry analysis of more than 111,000 pairwise comparisons between\r\nexcerpts. Teachers spanning grades 3-12 served as the raters for these\r\ncomparisons.’ A lower target value indicates a more challenging\r\ntext to read. The highest target score is equivalent to the 3rd-grade\r\nlevel, while the lowest target score is equivalent to the 12th-grade\r\nlevel. The purpose is to develop a model that predicts a readability\r\nscore for a given text to identify an appropriate reading level.\r\nIn the following weeks, we will talk a little bit about the\r\npre-trained language models (e.g., RoBerta). Our coverage of\r\nthis material will be at the surface level. We will primarily cover how\r\nwe obtain numerical vector representations (sentence embeddings) for\r\ngiven text input from a pre-trained language model using Python through\r\nR. Then, we will use the sentence embeddings as features to predict the\r\ntarget score in this dataset using various modeling frameworks.\r\nRecidivism\r\nThe Recidivism dataset comes from The National Institute of Justice’s\r\n(NIJ) Recidivism\r\nForecasting Challenge. The challenge aims to increase public safety\r\nand improve the fair administration of justice across the United States.\r\nThis challenge had three stages of prediction, and all three stages\r\nrequire modeling a binary outcome (recidivated vs. not recidivated in\r\nYear 1, Year 2, and Year 3). In this class, we will only work on the\r\nsecond stage and develop a model for predicting the probability of an\r\nindividual’s recidivism in the second year after initial release.\r\nYou can download the training dataset directly from the\r\ncompetition website, or from the course website. Either way, please\r\nread the Terms\r\nof Use at this link before working with this dataset.\r\n\r\n\r\nrecidivism <- read.csv(here('data/recidivism_full.csv'),header=TRUE)\r\n\r\nstr(recidivism)\r\n\r\n'data.frame':   25835 obs. of  54 variables:\r\n $ ID                                               : int  1 2 3 4 5 6 7 8 9 10 ...\r\n $ Gender                                           : chr  \"M\" \"M\" \"M\" \"M\" ...\r\n $ Race                                             : chr  \"BLACK\" \"BLACK\" \"BLACK\" \"WHITE\" ...\r\n $ Age_at_Release                                   : chr  \"43-47\" \"33-37\" \"48 or older\" \"38-42\" ...\r\n $ Residence_PUMA                                   : int  16 16 24 16 16 17 18 16 5 16 ...\r\n $ Gang_Affiliated                                  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n $ Supervision_Risk_Score_First                     : int  3 6 7 7 4 5 2 5 7 5 ...\r\n $ Supervision_Level_First                          : chr  \"Standard\" \"Specialized\" \"High\" \"High\" ...\r\n $ Education_Level                                  : chr  \"At least some college\" \"Less than HS diploma\" \"At least some college\" \"Less than HS diploma\" ...\r\n $ Dependents                                       : chr  \"3 or more\" \"1\" \"3 or more\" \"1\" ...\r\n $ Prison_Offense                                   : chr  \"Drug\" \"Violent/Non-Sex\" \"Drug\" \"Property\" ...\r\n $ Prison_Years                                     : chr  \"More than 3 years\" \"More than 3 years\" \"1-2 years\" \"1-2 years\" ...\r\n $ Prior_Arrest_Episodes_Felony                     : chr  \"6\" \"7\" \"6\" \"8\" ...\r\n $ Prior_Arrest_Episodes_Misd                       : chr  \"6 or more\" \"6 or more\" \"6 or more\" \"6 or more\" ...\r\n $ Prior_Arrest_Episodes_Violent                    : chr  \"1\" \"3 or more\" \"3 or more\" \"0\" ...\r\n $ Prior_Arrest_Episodes_Property                   : chr  \"3\" \"0\" \"2\" \"3\" ...\r\n $ Prior_Arrest_Episodes_Drug                       : chr  \"3\" \"3\" \"2\" \"3\" ...\r\n $ Prior_Arrest_Episodes_PPViolationCharges         : chr  \"4\" \"5 or more\" \"5 or more\" \"3\" ...\r\n $ Prior_Arrest_Episodes_DVCharges                  : logi  FALSE TRUE TRUE FALSE TRUE FALSE ...\r\n $ Prior_Arrest_Episodes_GunCharges                 : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n $ Prior_Conviction_Episodes_Felony                 : chr  \"3 or more\" \"3 or more\" \"3 or more\" \"3 or more\" ...\r\n $ Prior_Conviction_Episodes_Misd                   : chr  \"3\" \"4 or more\" \"2\" \"4 or more\" ...\r\n $ Prior_Conviction_Episodes_Viol                   : logi  FALSE TRUE TRUE FALSE TRUE FALSE ...\r\n $ Prior_Conviction_Episodes_Prop                   : chr  \"2\" \"0\" \"1\" \"3 or more\" ...\r\n $ Prior_Conviction_Episodes_Drug                   : chr  \"2 or more\" \"2 or more\" \"2 or more\" \"2 or more\" ...\r\n $ Prior_Conviction_Episodes_PPViolationCharges     : logi  FALSE TRUE FALSE FALSE FALSE FALSE ...\r\n $ Prior_Conviction_Episodes_DomesticViolenceCharges: logi  FALSE TRUE TRUE FALSE FALSE FALSE ...\r\n $ Prior_Conviction_Episodes_GunCharges             : logi  FALSE TRUE FALSE FALSE FALSE FALSE ...\r\n $ Prior_Revocations_Parole                         : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n $ Prior_Revocations_Probation                      : logi  FALSE FALSE FALSE TRUE FALSE FALSE ...\r\n $ Condition_MH_SA                                  : logi  TRUE FALSE TRUE TRUE TRUE FALSE ...\r\n $ Condition_Cog_Ed                                 : logi  TRUE FALSE TRUE TRUE TRUE FALSE ...\r\n $ Condition_Other                                  : logi  FALSE FALSE FALSE FALSE TRUE TRUE ...\r\n $ Violations_ElectronicMonitoring                  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n $ Violations_Instruction                           : logi  FALSE TRUE TRUE FALSE FALSE FALSE ...\r\n $ Violations_FailToReport                          : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n $ Violations_MoveWithoutPermission                 : logi  FALSE FALSE TRUE FALSE FALSE TRUE ...\r\n $ Delinquency_Reports                              : chr  \"0\" \"4 or more\" \"4 or more\" \"0\" ...\r\n $ Program_Attendances                              : chr  \"6\" \"0\" \"6\" \"6\" ...\r\n $ Program_UnexcusedAbsences                        : chr  \"0\" \"0\" \"0\" \"0\" ...\r\n $ Residence_Changes                                : chr  \"2\" \"2\" \"0\" \"3 or more\" ...\r\n $ Avg_Days_per_DrugTest                            : num  612 35.7 93.7 25.4 23.1 ...\r\n $ DrugTests_THC_Positive                           : num  0 0 0.333 0 0 ...\r\n $ DrugTests_Cocaine_Positive                       : num  0 0 0 0 0 0 0 0 NA 0 ...\r\n $ DrugTests_Meth_Positive                          : num  0 0 0.1667 0 0.0588 ...\r\n $ DrugTests_Other_Positive                         : num  0 0 0 0 0 0 0 0 NA 0 ...\r\n $ Percent_Days_Employed                            : num  0.489 0.425 0 1 0.204 ...\r\n $ Jobs_Per_Year                                    : num  0.448 2 0 0.719 0.929 ...\r\n $ Employment_Exempt                                : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n $ Recidivism_Within_3years                         : logi  FALSE TRUE TRUE FALSE TRUE FALSE ...\r\n $ Recidivism_Arrest_Year1                          : logi  FALSE FALSE FALSE FALSE TRUE FALSE ...\r\n $ Recidivism_Arrest_Year2                          : logi  FALSE FALSE TRUE FALSE FALSE FALSE ...\r\n $ Recidivism_Arrest_Year3                          : logi  FALSE TRUE FALSE FALSE FALSE FALSE ...\r\n $ Training_Sample                                  : int  1 1 1 1 1 0 1 0 1 1 ...\r\n\r\nThere are 25,835 observations in the training set and 54 variables,\r\nincluding a unique ID variable, four outcome variables (Recidivism in\r\nYear 1, Recidivism in Year 2, and Recidivism in Year 3, Recidivism\r\nwithin three years), and a filter variable to indicate whether an\r\nobservation was included in the training dataset or test dataset. The\r\nremaining 48 variables are potential predictive features. A complete\r\nlist of these variables can be found at this\r\nlink.\r\nWe will work on developing a model to predict the outcome variable\r\nRecidivism_Arrest_Year2 using the 48 potential predictive\r\nvariables. Before moving forward, we must remove the individuals who had\r\nalready recidivated in Year 1. As you can see below, about 29.9% of the\r\nindividuals recidivated in Year 1. I am removing these individuals from\r\nthe dataset.\r\n\r\n\r\ntable(recidivism$Recidivism_Arrest_Year1)\r\n\r\n\r\nFALSE  TRUE \r\n18111  7724 \r\n\r\nrecidivism2 <- recidivism[recidivism$Recidivism_Arrest_Year1 == FALSE,]\r\n\r\n\r\nI will also recode some variables before saving the new dataset for\r\nlater use in class.\r\nFirst, some variables in the dataset are coded as TRUE and FALSE.\r\nWhen these variables are imported into R, R automatically recognizes\r\nthem as logical variables. I will recode all these variables such that\r\nFALSE = 0 and TRUE = 1.\r\n\r\n\r\n# Find the columns recognized as logical\r\n\r\n  cols <- sapply(recidivism, is.logical)\r\n\r\n# Convert them to numeric 0s and 1s\r\n\r\n  recidivism2[,cols] <- lapply(recidivism2[,cols], as.numeric)\r\n\r\n\r\nSecond, the highest value for some variables are coded as 3\r\nor more, 4 or more, 10 or\r\nmore, etc. These variables can be considered as numeric, but R\r\nrecognizes them as character vectors due to phrase or\r\nmore for the highest value. We will recode these variables so\r\n‘X or more’ will be equal to X.\r\n\r\n\r\nrequire(dplyr)\r\n\r\n# Dependents\r\n\r\n  recidivism2$Dependents <- recode(recidivism2$Dependents,\r\n                                   '0'=0,\r\n                                   '1'=1,\r\n                                   '2'=2,\r\n                                   '3 or more'=3)\r\n\r\n# Prior Arrest Episodes Felony\r\n\r\n  recidivism2$Prior_Arrest_Episodes_Felony <- recode(recidivism2$Prior_Arrest_Episodes_Felony,\r\n                                                     '0'=0,\r\n                                                     '1'=1,\r\n                                                     '2'=2,\r\n                                                     '3'=3,\r\n                                                     '4'=4,\r\n                                                     '5'=5,\r\n                                                     '6'=6,\r\n                                                     '7'=7,\r\n                                                     '8'=8,\r\n                                                     '9'=9,\r\n                                                     '10 or more'=10)\r\n# Prior Arrest Episods Misd\r\n\r\n  recidivism2$Prior_Arrest_Episodes_Misd <- recode(recidivism2$Prior_Arrest_Episodes_Misd,\r\n                                                   '0'=0,\r\n                                                   '1'=1,\r\n                                                   '2'=2,\r\n                                                   '3'=3,\r\n                                                   '4'=4,\r\n                                                   '5'=5,\r\n                                                   '6 or more'=6)\r\n  \r\n# Prior Arrest Episodes Violent\r\n\r\n  recidivism2$Prior_Arrest_Episodes_Violent <- recode(recidivism2$Prior_Arrest_Episodes_Violent,\r\n                                                      '0'=0,\r\n                                                      '1'=1,\r\n                                                      '2'=2,\r\n                                                      '3 or more'=3)\r\n\r\n# Prior Arrest Episods Property\r\n\r\n  recidivism2$Prior_Arrest_Episodes_Property <- recode(recidivism2$Prior_Arrest_Episodes_Property,\r\n                                                       '0'=0,\r\n                                                       '1'=1,\r\n                                                       '2'=2,\r\n                                                       '3'=3,\r\n                                                       '4'=4,\r\n                                                       '5 or more'=5)\r\n  \r\n# Prior Arrest Episods Drug\r\n\r\n  recidivism2$Prior_Arrest_Episodes_Drug <- recode(recidivism2$Prior_Arrest_Episodes_Drug,\r\n                                                   '0'=0,\r\n                                                   '1'=1,\r\n                                                   '2'=2,\r\n                                                   '3'=3,\r\n                                                   '4'=4,\r\n                                                   '5 or more'=5) \r\n# Prior Arrest Episods PPViolationCharges\r\n\r\n  recidivism2$Prior_Arrest_Episodes_PPViolationCharges <- recode(recidivism2$Prior_Arrest_Episodes_PPViolationCharges,\r\n                                                                 '0'=0,\r\n                                                                 '1'=1,\r\n                                                                 '2'=2,\r\n                                                                 '3'=3,\r\n                                                                 '4'=4,\r\n                                                                 '5 or more'=5)  \r\n  \r\n# Prior Conviction Episodes Felony\r\n\r\n  recidivism2$Prior_Conviction_Episodes_Felony <- recode(recidivism2$Prior_Conviction_Episodes_Felony,\r\n                                                         '0'=0,\r\n                                                         '1'=1,\r\n                                                         '2'=2,\r\n                                                         '3 or more'=3)\r\n\r\n# Prior Conviction Episodes Misd\r\n\r\n  recidivism2$Prior_Conviction_Episodes_Misd <- recode(recidivism2$Prior_Conviction_Episodes_Misd,\r\n                                                       '0'=0,\r\n                                                       '1'=1,\r\n                                                       '2'=2,\r\n                                                       '3'=3,\r\n                                                       '4 or more'=4)\r\n  \r\n# Prior Conviction Episodes Prop\r\n\r\n  recidivism2$Prior_Conviction_Episodes_Prop <- recode(recidivism2$Prior_Conviction_Episodes_Prop,\r\n                                                       '0'=0,\r\n                                                       '1'=1,\r\n                                                       '2'=2,\r\n                                                       '3 or more'=3)\r\n\r\n# Prior Conviction Episodes Drug\r\n\r\n  recidivism2$Prior_Conviction_Episodes_Drug <- recode(recidivism2$Prior_Conviction_Episodes_Drug,\r\n                                                       '0'=0,\r\n                                                       '1'=1,\r\n                                                       '2 or more'=2)\r\n\r\n# Delinquency Reports\r\n\r\n  recidivism2$Delinquency_Reports <- recode(recidivism2$Delinquency_Reports,\r\n                                            '0'=0,\r\n                                            '1'=1,\r\n                                            '2'=2,\r\n                                            '3'=3,\r\n                                            '4 or more'=4)\r\n\r\n# Program Attendances\r\n\r\n  recidivism2$Program_Attendances <- recode(recidivism2$Program_Attendances,\r\n                                            '0'=0,\r\n                                            '1'=1,\r\n                                            '2'=2,\r\n                                            '3'=3,\r\n                                            '4'=4,\r\n                                            '5'=5,\r\n                                            '6'=6,\r\n                                            '7'=7,\r\n                                            '8'=8,\r\n                                            '9'=9,\r\n                                            '10 or more'=10)\r\n\r\n# Program Unexcused Absences\r\n\r\n  recidivism2$Program_UnexcusedAbsences <- recode(recidivism2$Program_UnexcusedAbsences,\r\n                                                  '0'=0,\r\n                                                  '1'=1,\r\n                                                  '2'=2,\r\n                                                  '3 or more'=3)\r\n\r\n# Residence Changes\r\n\r\n  recidivism2$Residence_Changes <- recode(recidivism2$Residence_Changes,\r\n                                          '0'=0,\r\n                                          '1'=1,\r\n                                          '2'=2,\r\n                                          '3 or more'=3)  \r\n#############################################################\r\n  \r\nstr(recidivism2)  \r\n\r\n'data.frame':   18111 obs. of  54 variables:\r\n $ ID                                               : int  1 2 3 4 6 7 8 11 13 15 ...\r\n $ Gender                                           : chr  \"M\" \"M\" \"M\" \"M\" ...\r\n $ Race                                             : chr  \"BLACK\" \"BLACK\" \"BLACK\" \"WHITE\" ...\r\n $ Age_at_Release                                   : chr  \"43-47\" \"33-37\" \"48 or older\" \"38-42\" ...\r\n $ Residence_PUMA                                   : int  16 16 24 16 17 18 16 5 18 5 ...\r\n $ Gang_Affiliated                                  : num  0 0 0 0 0 0 0 0 0 0 ...\r\n $ Supervision_Risk_Score_First                     : int  3 6 7 7 5 2 5 3 3 7 ...\r\n $ Supervision_Level_First                          : chr  \"Standard\" \"Specialized\" \"High\" \"High\" ...\r\n $ Education_Level                                  : chr  \"At least some college\" \"Less than HS diploma\" \"At least some college\" \"Less than HS diploma\" ...\r\n $ Dependents                                       : num  3 1 3 1 0 2 3 1 1 1 ...\r\n $ Prison_Offense                                   : chr  \"Drug\" \"Violent/Non-Sex\" \"Drug\" \"Property\" ...\r\n $ Prison_Years                                     : chr  \"More than 3 years\" \"More than 3 years\" \"1-2 years\" \"1-2 years\" ...\r\n $ Prior_Arrest_Episodes_Felony                     : num  6 7 6 8 4 10 6 3 8 9 ...\r\n $ Prior_Arrest_Episodes_Misd                       : num  6 6 6 6 0 6 6 6 4 3 ...\r\n $ Prior_Arrest_Episodes_Violent                    : num  1 3 3 0 1 1 3 2 0 2 ...\r\n $ Prior_Arrest_Episodes_Property                   : num  3 0 2 3 3 5 1 1 5 2 ...\r\n $ Prior_Arrest_Episodes_Drug                       : num  3 3 2 3 0 1 2 1 2 4 ...\r\n $ Prior_Arrest_Episodes_PPViolationCharges         : num  4 5 5 3 0 5 5 3 1 4 ...\r\n $ Prior_Arrest_Episodes_DVCharges                  : num  0 1 1 0 0 0 0 1 0 0 ...\r\n $ Prior_Arrest_Episodes_GunCharges                 : num  0 0 0 0 0 1 0 0 0 1 ...\r\n $ Prior_Conviction_Episodes_Felony                 : num  3 3 3 3 1 3 1 0 1 3 ...\r\n $ Prior_Conviction_Episodes_Misd                   : num  3 4 2 4 0 1 4 3 0 2 ...\r\n $ Prior_Conviction_Episodes_Viol                   : num  0 1 1 0 0 0 1 0 0 1 ...\r\n $ Prior_Conviction_Episodes_Prop                   : num  2 0 1 3 2 3 0 0 2 1 ...\r\n $ Prior_Conviction_Episodes_Drug                   : num  2 2 2 2 0 0 2 0 1 1 ...\r\n $ Prior_Conviction_Episodes_PPViolationCharges     : num  0 1 0 0 0 1 1 1 0 1 ...\r\n $ Prior_Conviction_Episodes_DomesticViolenceCharges: num  0 1 1 0 0 0 0 0 0 0 ...\r\n $ Prior_Conviction_Episodes_GunCharges             : num  0 1 0 0 0 1 0 0 0 0 ...\r\n $ Prior_Revocations_Parole                         : num  0 0 0 0 0 0 0 1 0 0 ...\r\n $ Prior_Revocations_Probation                      : num  0 0 0 1 0 0 0 0 0 0 ...\r\n $ Condition_MH_SA                                  : num  1 0 1 1 0 0 0 1 0 1 ...\r\n $ Condition_Cog_Ed                                 : num  1 0 1 1 0 0 1 1 0 1 ...\r\n $ Condition_Other                                  : num  0 0 0 0 1 0 0 0 0 1 ...\r\n $ Violations_ElectronicMonitoring                  : num  0 0 0 0 0 0 0 1 0 0 ...\r\n $ Violations_Instruction                           : num  0 1 1 0 0 0 0 1 0 0 ...\r\n $ Violations_FailToReport                          : num  0 0 0 0 0 0 0 0 0 0 ...\r\n $ Violations_MoveWithoutPermission                 : num  0 0 1 0 1 0 0 0 0 0 ...\r\n $ Delinquency_Reports                              : num  0 4 4 0 0 0 0 0 0 0 ...\r\n $ Program_Attendances                              : num  6 0 6 6 0 0 0 9 0 6 ...\r\n $ Program_UnexcusedAbsences                        : num  0 0 0 0 0 0 0 2 0 0 ...\r\n $ Residence_Changes                                : num  2 2 0 3 3 1 0 2 1 1 ...\r\n $ Avg_Days_per_DrugTest                            : num  612 35.7 93.7 25.4 474.6 ...\r\n $ DrugTests_THC_Positive                           : num  0 0 0.333 0 0 ...\r\n $ DrugTests_Cocaine_Positive                       : num  0 0 0 0 0 0 0 0 0 0 ...\r\n $ DrugTests_Meth_Positive                          : num  0 0 0.167 0 0 ...\r\n $ DrugTests_Other_Positive                         : num  0 0 0 0 0 ...\r\n $ Percent_Days_Employed                            : num  0.489 0.425 0 1 0.674 ...\r\n $ Jobs_Per_Year                                    : num  0.448 2 0 0.719 0.308 ...\r\n $ Employment_Exempt                                : num  0 0 0 0 0 0 0 1 0 1 ...\r\n $ Recidivism_Within_3years                         : num  0 1 1 0 0 1 0 1 0 0 ...\r\n $ Recidivism_Arrest_Year1                          : num  0 0 0 0 0 0 0 0 0 0 ...\r\n $ Recidivism_Arrest_Year2                          : num  0 0 1 0 0 0 0 1 0 0 ...\r\n $ Recidivism_Arrest_Year3                          : num  0 1 0 0 0 1 0 0 0 0 ...\r\n $ Training_Sample                                  : int  1 1 1 1 0 1 0 1 1 0 ...\r\n\r\nNow, we export the final version of the dataset.\r\n\r\n\r\nwrite.csv(recidivism2, \r\n          here('data/recidivism_y1 removed and recoded.csv'),\r\n          row.names = FALSE)\r\n\r\n\r\nIn future weeks, we will work with this version of the dataset.\r\nInstalling\r\nReticulate, Miniconda, and Sentence Transformers\r\nYou will need to install the reticulate package and\r\nsentence_transformers module for the following weeks. You\r\ncan run the following code in your computer to get prepared for the\r\nfollowing weeks. Note that you only have to run the following code once\r\nto install the necessary packages.\r\nIf you are having troubles about installing these packages in your\r\ncomputer, I highly recommend using a Kaggle R notebook which these\r\npackages are already installed (I will give more information about this\r\nin class).\r\n\r\n\r\n# Install the reticulate package\r\n\r\ninstall.packages(pkgs = 'reticulate',\r\n                 dependencies = TRUE)\r\n\r\n\r\n\r\n# Install Miniconda\r\n\r\ninstall_miniconda()\r\n\r\n\r\nOnce you install the reticulate package, run the following code to\r\nget python configurations and make sure everything is properly\r\ninstalled.\r\n\r\n\r\n# Load the reticulate package\r\n\r\nrequire(reticulate)\r\n\r\nconda_list()\r\n\r\n          name\r\n1    Anaconda3\r\n2 r-reticulate\r\n                                                                          python\r\n1                                       C:\\\\Users\\\\cengiz\\\\Anaconda3\\\\python.exe\r\n2 C:\\\\Users\\\\cengiz\\\\AppData\\\\Local\\\\r-miniconda\\\\envs\\\\r-reticulate\\\\python.exe\r\n\r\nYou should see r-reticulate under the name column as one\r\nof your virtual Python environment. Finally, you will also need to\r\ninstall the sentence transformers module. The following code will\r\ninstall the sentence transformers module to the virtual Python\r\nenvironment r-reticulate.\r\n\r\n\r\n# Install the sentence transformer module \r\n\r\nuse_condaenv('r-reticulate')\r\n\r\nconda_install(envname  = 'r-reticulate',\r\n              packages = 'sentence_transformers',\r\n              pip      = TRUE)\r\n\r\n[1] \"sentence_transformers\"\r\n\r\n  # try pip=FALSE, if it gives an error message\r\n\r\n\r\nOnce you install the Python packages using the code above, you can\r\nrun the following code. If you are seeing the same output as below, you\r\nshould be all set to explore some very exciting NLP tools using the\r\nReadability dataset.\r\n\r\n\r\nrequire(reticulate)\r\n\r\n# Import the sentence transformer module\r\n\r\nreticulate::import('sentence_transformers')\r\n\r\nModule(sentence_transformers)\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-07-06T19:21:18-07:00"
    },
    {
      "path": "lecture-2a.html",
      "title": "Data Preprocessing I (Continuous and Categorical Data)",
      "author": [
        {
          "name": "Cengiz Zopluoglu",
          "url": {}
        }
      ],
      "date": "07/06/2022",
      "contents": "\r\n\r\nContents\r\n1. Scales of\r\nMeasurement and Types of Variables\r\n2. Processing\r\nCategorical Variables\r\n2.1 One-hot encoding\r\n(Dummy Variables)\r\n2.2. Label encoding\r\n2.3. Polynomial Contrasts\r\n\r\n3. Processing Cyclic\r\nVariables\r\n4. Processing Continuous\r\nVariables\r\n4.1. Centering and\r\nScaling (Standardization)\r\n4.2. Box-Cox\r\ntransformation\r\n4.3. Logit Transformation\r\n4.4. Basis Expansions\r\n\r\n5. Handling Missing Data\r\n5.1.\r\nCreating an indicator variable for missingness\r\n5.2. Imputation\r\n\r\n6. Wrapping-up\r\nusing the recipes package\r\n\r\n[Updated: Wed, Jul 06, 2022 - 19:21:18 ]\r\n1. Scales of\r\nMeasurement and Types of Variables\r\nIt is important to understand the nature of variables and how they\r\nwere measured and represented in a dataset. In social sciences, in\r\nparticular psychology, there is a methodological consensus about the\r\nframework provided by Stevens (1946), also see\r\nMichell\r\n(2002) for an in-depth discussion. According to Stevens’ definition,\r\nthere are four levels of measurement: nominal, ordinal, interval, and\r\nratio. Whether a variable has a nominal, ordinal, interval, or ratio\r\nscale depends on the character of the empirical operations performed\r\nwhile constructing the variable.\r\nNominal scale: Variables with a nominal scale cannot be\r\nmeaningfully added, subtracted, divided, or multiplied. Also, there is\r\nno hierarchical order among the assigned values. Most variables\r\ncontaining labels for individual observations can be considered nominal,\r\ne.g., hair color, city, state, and ethnicity.\r\nOrdinal scale: Variables with an ordinal scale also represent\r\nlabels; however, there is a meaningful hierarchy among the assigned\r\nvalues. For instance, if a variable is coded as Low, Medium, and High,\r\nthey are simply labels. Still, we know that High represents something\r\nmore than Medium and Medium represents something higher than Low (High\r\n> Medium > Low). On the other hand, the distance between the\r\nassigned values does not necessarily represent the same amount of\r\ndifference. Other examples of variables that can be considered as\r\nordinal are letter grades (A-F), scores from Likert type items (Strongly\r\nagree, agree, disagree, strongly disagree), education status(high\r\nschool, college, master’s, Ph.D.), cancer stage (stage1, stage2,\r\nstage3), order of finish in a competition (1st, 2nd, 3rd).\r\nInterval scale: Variables with an ordinal scale represent\r\nquantities with equal measurement units but don’t have an absolute zero\r\npoint. For instance, a typical example of an interval scale is\r\ntemperature measured on the Fahrenheit scale. The difference between 20F\r\nand 30F is the same as between 60F and 70F. However, 0F does not\r\nindicate the absence of heat.\r\nRatio scale: Variables with a ratio scale represent quantities\r\nwith equal measurement units and have an absolute zero. Due to the\r\nexistence of an absolute zero point that represents ‘nothing,’ the ratio\r\nof measurements is also meaningful. Typical examples are height, mass,\r\ndistance, and length.\r\nBelow table provides a summary of properties for each scale.\r\n\r\nIndicating Difference\r\nIndicating Direction of Difference\r\nIndicating Amount of Difference\r\nHas absolute zero\r\nNominal\r\nX\r\n\r\n\r\n\r\nOrdinal\r\nX\r\nX\r\n\r\n\r\nInterval\r\nX\r\nX\r\nX\r\n\r\nRatio\r\nX\r\nX\r\nX\r\nX\r\nIn this class, we classify the variables in two types:\r\nCategorical and Continuous. The\r\nvariables with a nominal or ordinal scale are considered as\r\nCategorical and the variables with an interval or ratio\r\nscale are considered as Continuous.\r\n2. Processing Categorical\r\nVariables\r\nWhen categorical predictors are in a dataset, it is essential to\r\ntransform them into numerical codes. When encoding categorical\r\npredictors, we try to preserve as much information as possible from\r\ntheir labels. Therefore, different strategies may be used for\r\ncategorical variables with different ordinal scales.\r\n2.1 One-hot encoding (Dummy\r\nVariables)\r\nA dummy variable is a synthetic variable with two outcomes (0 and 1)\r\nrepresenting a group membership. When there is a nominal variable with\r\nN levels, it is typical to create N dummy variables to\r\nrepresent the information in the nominal variable. Each dummy variable\r\nrepresents membership to one of the levels in the nominal variable.\r\nThese dummy variables can be used as features in predictive models.\r\nIn its simplest case, consider the variable Race in the\r\nRecidivism dataset with two levels: Black and White. We can create two\r\ndummy variables: the first dummy variable represents whether or not an\r\nindividual is Black, and the second dummy variable represents whether or\r\nnot the individual is White.\r\n\r\nDummy Variable 1\r\nDummy Variable 2\r\nBlack\r\n1\r\n0\r\nWhite\r\n0\r\n1\r\nLet’s consider another example from the Recidivism dataset. Variable\r\nPrison_Offense has five categories: Violent/Sex,\r\nViolent/Non-Sex, Property, Drug, and Other. We can create five dummy\r\nvariables using the following coding scheme.\r\n\r\nDummy Variable 1\r\nDummy Variable 2\r\nDummy Variable 3\r\nDummy Variable 4\r\nDummy Variable 5\r\nViolent/Sex\r\n1\r\n0\r\n0\r\n0\r\n0\r\nViolent/Non-Sex\r\n0\r\n1\r\n0\r\n0\r\n0\r\nProperty\r\n0\r\n0\r\n1\r\n0\r\n0\r\nDrug\r\n0\r\n0\r\n0\r\n1\r\n0\r\nOther\r\n0\r\n0\r\n0\r\n0\r\n1\r\nNote that Prison_Offence is missing for several\r\nobservations. You can fill in the missing values before creating dummy\r\nvariables using one of the methods we will discuss later. Alternatively,\r\nwe can define Missing as the sixth category to preserve that\r\ninformation.\r\n\r\nDummy Variable 1\r\nDummy Variable 2\r\nDummy Variable 3\r\nDummy Variable 4\r\nDummy Variable 5\r\nDummy Variable 6\r\nViolent/Sex\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\nViolent/Non-Sex\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\nProperty\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\nDrug\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\nOther\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\nMissing\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\nIn some cases, when you have a geographical location with a\r\nreasonable number of categories (e.g., counties or cities in a state,\r\nschools in a district), you can also create dummy variables to represent\r\nthis information. In our case, the Recidivism dataset has a variable\r\ncalled Residence_PUMA indicating Public\r\nUse Microdata Area (PUMA) for the residence address at the time\r\nindividual was released. This variable has 25 unique codes (1-25);\r\nhowever, these numbers are just labels. So, one can create 25 different\r\ndummy variables to represent 25 different PUMAs.\r\n\r\n\r\nNOTE\r\n\r\nWhen you fit a typical regression model without regularization using\r\nordinary least-squares (OLS), a typical practice is to drop a dummy\r\nvariable for one of the levels. So, for instance, if there are\r\nN levels for a nominal variable, you only have to create\r\n(N-1) dummy variables, as the Nth one has redundant\r\ninformation. The information regarding the excluded category is\r\nrepresented in the intercept term. It creates a problem when you put all\r\nN dummy variables into the model because the OLS procedure\r\ntries to invert a singular matrix, and you will likely get an error\r\nmessage.\r\nOn the other hand, this is not an issue when you fit a regularized\r\nregression model, which will be the case in this class. Therefore, you\r\ndo not need to drop one of the dummy variables and can include all of\r\nthem in the analysis. In fact, it may be beneficial to keep the dummy\r\nvariables for all categories in the model when regularization is used in\r\nthe regression. Otherwise, the model may produce different predictions\r\ndepending on which category is excluded.\r\n\r\n2.2. Label encoding\r\nWhen the variable of interest is ordinal, and there is a hierarchy\r\namong the levels, we can still use one-hot encoding to create a set of\r\ndummy variables to represent the information in the ordinal variable.\r\nHowever, dummy variables will not provide information regarding the\r\ncategories’ hierarchy.\r\nFor instance, consider the variable Age_At_Release in\r\nthe Recidivism dataset. It is coded as 7 different age intervals in the\r\ndataset: 18-22, 23-27, 28-32, 33-37, 38-42, 43-47, 48 or older. One can\r\ncreate seven dummy variables to represent each category in this\r\nvariable. Alternatively, one can assign a numeric variable to each\r\ncategory that may represent the information in these categories. For\r\ninstance, one can assign numbers from 1 to 7, respectively. Or, one can\r\nchoose the midpoint of each interval to represent each category (e.g.,\r\n20,25,31,35,40,45,60).\r\nAnother example would be the variable Education Level in\r\nthe Recidivism dataset. It has three levels: At least some college, High\r\nSchool Diploma, and Less than a High School diploma. One can create\r\nthree dummy variables to represent each level. Alternatively, one can\r\nassign 1, 2, and 3, respectively. Or, one can assign a number for the\r\napproximate years of schooling for each level, such as 9, 12, and\r\n15.\r\n2.3. Polynomial Contrasts\r\nAnother way of encoding an ordinal variable is to use polynomial\r\ncontrasts. The polynomial contrasts may be helpful if one wants to\r\nexplore whether or not there is a linear, quadratic, cubic, etc., the\r\nrelationship between the predictor variable and outcome variable. You\r\ncan use the stat::poly() function in R to obtain the set of\r\npolynomial contrasts. If there are N levels in an ordinal\r\nvariable, you can get polynomials up to degree N-1.\r\nFor instance, suppose you have an ordinal variable with three levels:\r\nLow, Medium, and High. Then, stat::poly(x=1:3,degree=2)\r\nwill return the polynomial contrasts for the linear and quadratic terms.\r\nNotice that the input for the poly function is a vector of\r\nnumeric values corresponding to the ordinal variable levels and the\r\ndegree of the requested polynomial terms. For this example, it creates\r\ntwo sets of vectors to represent this ordinal variable. Note that the\r\nsum of the squares within each column equals 1, and the dot product of\r\nthe contrast vectors equals 0. In other words, the polynomial terms\r\nrepresent a set of orthonormal vectors.\r\n\r\nLinear\r\nQuadratic\r\nLow\r\n-0.707\r\n0.408\r\nMedium\r\n0\r\n-0.816\r\nHigh\r\n0.707\r\n0.408\r\n\r\n\r\nctr <- poly(1:3,2)\r\nround(ctr,3)\r\n\r\n          1      2\r\n[1,] -0.707  0.408\r\n[2,]  0.000 -0.816\r\n[3,]  0.707  0.408\r\nattr(,\"coefs\")\r\nattr(,\"coefs\")$alpha\r\n[1] 2 2\r\n\r\nattr(,\"coefs\")$norm2\r\n[1] 1.0000000 3.0000000 2.0000000 0.6666667\r\n\r\nattr(,\"degree\")\r\n[1] 1 2\r\nattr(,\"class\")\r\n[1] \"poly\"   \"matrix\"\r\n\r\nsum(ctr[,1]^2)\r\n\r\n[1] 1\r\n\r\nsum(ctr[,2]^2)\r\n\r\n[1] 1\r\n\r\nsum(ctr[,1]*ctr[,2])\r\n\r\n[1] 0.00000000000000006410345\r\n\r\n\r\n\r\n\r\nIf we consider the variable Age_At_Release with 7\r\ndifferent levels, then we can have polynomial terms up to the 6th\r\ndegree.\r\n\r\n\r\nctr <- poly(1:7,6)\r\nround(ctr,3)\r\n\r\n          1      2      3      4      5      6\r\n[1,] -0.567  0.546 -0.408  0.242 -0.109  0.033\r\n[2,] -0.378  0.000  0.408 -0.564  0.436 -0.197\r\n[3,] -0.189 -0.327  0.408  0.081 -0.546  0.493\r\n[4,]  0.000 -0.436  0.000  0.483  0.000 -0.658\r\n[5,]  0.189 -0.327 -0.408  0.081  0.546  0.493\r\n[6,]  0.378  0.000 -0.408 -0.564 -0.436 -0.197\r\n[7,]  0.567  0.546  0.408  0.242  0.109  0.033\r\nattr(,\"coefs\")\r\nattr(,\"coefs\")$alpha\r\n[1] 4 4 4 4 4 4\r\n\r\nattr(,\"coefs\")$norm2\r\n[1]   1.0000   7.0000  28.0000  84.0000 216.0000 452.5714 685.7143\r\n[8] 561.0390\r\n\r\nattr(,\"degree\")\r\n[1] 1 2 3 4 5 6\r\nattr(,\"class\")\r\n[1] \"poly\"   \"matrix\"\r\n\r\n\r\n\r\n\r\n\r\n\r\nNOTE\r\n\r\nThere are other ways of encoding nominal and ordinal variables (e.g.,\r\nHelmert contrasts), or one can come up with their own set of contrast\r\nvalues. When the goal of analysis is inference and you run analysis to\r\nrespond to a specific research question, your research question\r\ntypically dictates the type of encoding to use. You choose a coding\r\nscheme that provides you the most interpretable coefficients to respond\r\nto your research question.\r\nOn the other hand, when the goal of analysis is prediction, how you\r\nencode your categorical variable does not make much difference. In fact,\r\nthey provide very similar predictions. Below, I provide an example using\r\nAge_at_Release variable to predict the outcome using different coding\r\nschemes and report the average squared error of predictions from a\r\nlogistic regression model.\r\n\r\nEncoding\r\nAverage Squared Error\r\nIntercept-Only (NULL)\r\n0.1885789\r\nDummy Variables\r\n0.1861276\r\nLabel Encoding\r\n0.1861888\r\nPolynomial Contrasts\r\n0.1861276\r\nHelmert Contrasts\r\n0.1861276\r\n\r\nNotice that one-hot encoding, polynomial contrasts, and helmert\r\ncontrasts have identical performance. In fact, they yield the exact same\r\npredicted value for observations. Moreover, a simple label encoding with\r\na single constructed variable does (almost) as well as other encoding\r\ntypes with multiple constructed variables.\r\n\r\n3. Processing Cyclic Variables\r\nThere are sometimes variables that are cyclic by nature (e.g.,\r\nmonths, days, hour), and a type of encoding that represents their cyclic\r\nnature may be the most meaningful way to represent them instead of\r\nnumerical or categorical encoding. One way to achieve this is to create\r\ntwo new variables using a sine and cosine transformation as the\r\nfollowing:\r\n\\[x_{1} = sin(\\frac{2 \\pi\r\nx}{max(x)}),\\] \\[x_{2} = cos(\\frac{2\r\n\\pi x}{max(x)}).\\]\r\nFor instance, suppose one of the variables in a dataset is the day of\r\nthe week. We can represent its cyclic nature using the two variables as\r\ndefined the following. Once the corresponding coordinates are calculated\r\nfor each day of the week, the single variable that represents days in\r\nthe data can be replaced with these two variables representing their\r\ncoordinates in a unit circle.\r\n\r\n\r\nd <- data.frame(days = c('Mon','Tue','Wed','Thu','Fri','Sat','Sun'),\r\n                x = 1:7)\r\n\r\nd$x1 <- sin((2*pi*d$x)/7)\r\nd$x2 <- cos((2*pi*d$x)/7)\r\n\r\nd\r\n\r\n  days x                        x1         x2\r\n1  Mon 1  0.7818314824680298036341  0.6234898\r\n2  Tue 2  0.9749279121818236193420 -0.2225209\r\n3  Wed 3  0.4338837391175582314240 -0.9009689\r\n4  Thu 4 -0.4338837391175580093794 -0.9009689\r\n5  Fri 5 -0.9749279121818236193420 -0.2225209\r\n6  Sat 6 -0.7818314824680299146564  0.6234898\r\n7  Sun 7 -0.0000000000000002449213  1.0000000\r\n\r\n\r\n\r\n\r\nWe can apply the same concept to any cyclic variable. Here is another\r\nexample for the time of day.\r\n\r\n\r\nd <- data.frame(hour = 1:24)\r\n\r\nd$x1 <- sin((2*pi*d$hour)/24)\r\nd$x2 <- cos((2*pi*d$hour)/24)\r\n\r\nd\r\n\r\n   hour                        x1                         x2\r\n1     1  0.2588190451025207394764  0.96592582628906831221371\r\n2     2  0.4999999999999999444888  0.86602540378443870761060\r\n3     3  0.7071067811865474617150  0.70710678118654757273731\r\n4     4  0.8660254037844385965883  0.50000000000000011102230\r\n5     5  0.9659258262890683122137  0.25881904510252073947640\r\n6     6  1.0000000000000000000000  0.00000000000000006123032\r\n7     7  0.9659258262890683122137 -0.25881904510252062845410\r\n8     8  0.8660254037844387076106 -0.49999999999999977795540\r\n9     9  0.7071067811865475727373 -0.70710678118654746171501\r\n10   10  0.4999999999999999444888 -0.86602540378443870761060\r\n11   11  0.2588190451025210170322 -0.96592582628906820119141\r\n12   12  0.0000000000000001224606 -1.00000000000000000000000\r\n13   13 -0.2588190451025207949876 -0.96592582628906831221371\r\n14   14 -0.4999999999999997224442 -0.86602540378443881863291\r\n15   15 -0.7071067811865471286481 -0.70710678118654790580422\r\n16   16 -0.8660254037844383745437 -0.50000000000000044408921\r\n17   17 -0.9659258262890683122137 -0.25881904510252062845410\r\n18   18 -1.0000000000000000000000 -0.00000000000000018369095\r\n19   19 -0.9659258262890684232360  0.25881904510252029538719\r\n20   20 -0.8660254037844385965883  0.50000000000000011102230\r\n21   21 -0.7071067811865476837596  0.70710678118654735069271\r\n22   22 -0.5000000000000004440892  0.86602540378443837454370\r\n23   23 -0.2588190451025215721437  0.96592582628906809016911\r\n24   24 -0.0000000000000002449213  1.00000000000000000000000\r\n\r\n\r\n\r\n\r\n4. Processing Continuous\r\nVariables\r\n4.1. Centering and\r\nScaling (Standardization)\r\nCentering a variable is done by subtracting the variable’s mean from\r\nevery variable’s value, ensuring that the mean of the centered variable\r\nequals zero. Scaling a variable is dividing the value of each\r\nobservation by the variable’s standard deviation. When centering and\r\nscaling are both applied, it is called standardization.\r\nWhen we standardize a variable, we ensure that its mean is equal to\r\nzero and variance is equal to 1. Standardizing outcome and predictor\r\nvariables may be critical and necessary for specific models (e.g.,\r\nK-nearest neighbor, support vector machines, penalized regression), but\r\nit is not always necessary for other models (e.g., decision tree\r\nmodels).\r\n\r\n\r\nNOTE\r\n\r\nStandardizing a variable only changes the first and second moments of\r\na distribution (mean and variance); however, it doesn’t change the third\r\nand fourth moments of a distribution (skewness and kurtosis). Notice\r\nthat the skewness and kurtosis for both the original and the\r\nstandardized variable are 3.64 and 18.24, respectively. We only change\r\nthe mean to zero and variance to one by standardizing a variable.\r\n\r\n4.2. Box-Cox transformation\r\nVariables with extreme skewness and kurtosis may deteriorate the\r\nmodel performance for certain types of models. Therefore, it may\r\nsometimes be useful to transform a variable with extreme skewness and\r\nkurtosis such that its distribution approximates to a normal\r\ndistribution. Box-Cox transformation is a method to find an optimal\r\nparameter of \\(\\lambda\\) to apply the\r\nfollowing transformation:\r\n\\[y^{(\\lambda)}=\\left\\{\\begin{matrix}\r\n\\frac{y^{\\lambda}-1}{\\lambda} & , \\lambda \\neq 0 \\\\\r\n& \\\\\r\nln(y) & , \\lambda = 0\r\n\\end{matrix}\\right.\\]\r\nBelow is an example of transforming the a right-skewed variable using\r\nthe boxcox function from the bestNormalize\r\npackage. Notice that the skewness and the kurtosis for the transformed\r\nvariable are 0 and -0.02, respectively.\r\n\r\n\r\nrequire(bestNormalize)\r\nrequire(psych)\r\n\r\n\r\nold <- rbeta(1000,1,1000)\r\n\r\nfit <- boxcox(old,standardize=FALSE)\r\nfit\r\n\r\nNon-Standardized Box Cox Transformation with 1000 nonmissing obs.:\r\n Estimated statistics:\r\n - lambda = 0.2771826 \r\n - mean (before standardization) = -3.124325 \r\n - sd (before standardization) = 0.1503301 \r\n\r\nnew <- predict(fit)\r\n \r\ndescribe(old)\r\n\r\n   vars    n mean sd median trimmed mad min  max range skew kurtosis\r\nX1    1 1000    0  0      0       0   0   0 0.01  0.01 2.06     6.21\r\n   se\r\nX1  0\r\n\r\ndescribe(new)\r\n\r\n   vars    n  mean   sd median trimmed  mad   min   max range  skew\r\nX1    1 1000 -3.12 0.15  -3.12   -3.12 0.15 -3.51 -2.68  0.83 -0.04\r\n   kurtosis se\r\nX1    -0.24  0\r\n\r\n\r\n\r\n\r\n\r\n\r\nNOTE\r\n\r\nBox-Cox transformation can be used only for variables with positive\r\nvalues. Therefore, it is a good idea to first implement the Box-Cox\r\ntransformation, and then standardize a variable if both procedures will\r\nbe applied to a variable. If there is a variable with negative values or\r\na mix of both positive and negative values, the Yeo-Johnson\r\ntransformation is available as an extension of the Box-Cox\r\ntransformation. See this link for more\r\ninformation. The function yeojohnson is available in the\r\nbestNormalizer package to implement the Yeo-Johnson\r\ntransformation (See, ?bestNormalizer::yeojohnson).\r\n\r\n4.3. Logit Transformation\r\nWhen a variable is a proportion bounded between 0 and 1, the logit\r\ntransformation can be applied such that\r\n\\[\\pi^{*} =\r\nln(\\frac{\\pi}{1-\\pi}),\\]\r\nwhere \\(\\pi\\) represents a\r\nproportion. This may be particularly useful when your outcome variable\r\nis a proportion bounded between 0 and 1. When a linear model is used to\r\nmodel an outcome bounded between 0 and 1, the model predictions may\r\nexceed the reasonable range of values (predictions equal to less than\r\nzero or greater than one). Logit transformation scales variables such\r\nthat the range of values becomes \\(-\\infty\\) and \\(\\infty\\) on the logit scale. One can build\r\na model to predict logit (\\(\\pi^*\\))\r\ninstead of proportion (\\(\\pi\\)) and\r\nthen ensure that the model predictions are bounded between 0 and 1 on\r\nthe original proportion scale after a simple reverse operation for\r\npredicted values.\r\n\\[\\pi = \\frac{e^{\\pi^*}}{1+e^{\\pi^*}}\r\n\\]\r\nOne caveat of using logit transformation is that it is not defined\r\nfor 0 and 1. So, when you have values in the dataset exactly equal to 0\r\nor 1, logit transformation will return either \\(-\\infty\\) and \\(\\infty\\). In these situations, we may add\r\nor substract a very tiny constant (e.g., .0001) to force the\r\ntransformation to return a numerical value.\r\n\\[\\pi^{*} = ln(\\frac{\\pi}{1-\\pi}) =\r\nln(\\frac{0}{1-0}) = -\\infty\\] \\[\\pi^{*} = ln(\\frac{\\pi}{1-\\pi}) =\r\nln(\\frac{1}{1-1}) = \\infty\\]\r\nBelow is an example of logit transformation for a randomly generated\r\nvariable.\r\n\r\n\r\nold <- rbeta(1000,1,1000)\r\n\r\nnew <- log(old/(1-old))\r\n\r\n\r\n\r\n\r\n\r\n4.4. Basis Expansions\r\nBasis expansions are useful to address nonlinearity between a\r\ncontinuous predictor variable and outcome variable. Using the basis\r\nexpansions, one can create a set of feature variables using a nonlinear\r\nfunction of a variable x, \\(\\phi(x)\\). One simply replaces the original\r\nvariable x with the new variables obtained from \\(\\phi(x)\\). For continuous predictors, the\r\nmost commonly used expansions are polynomial basis expansions. The \\(n^{th}\\) degree polynomial basis expansion\r\ncan be represented by\r\n\\[\\phi(x) = \\beta_1x + \\beta_2x^2 +\r\n\\beta_3x^3 + ... + \\beta_nx^n .\\]\r\nSuppose we have 100 observation from a random normal variable\r\nx. The third degree polynomial basis expansion (cubic basis\r\nexpansion) can be found using the poly function as the\r\nfollowing.\r\n\r\n\r\nset.seed(654)\r\n\r\nx <- rnorm(100,0,1)\r\n\r\nhead(x)\r\n\r\n[1] -0.76031762 -0.38970450  1.68962523 -0.09423560  0.09530146\r\n[6]  0.81727228\r\n\r\nhead(poly(x,degree=3))\r\n\r\n                1           2            3\r\n[1,] -0.070492258 -0.06612854  0.056003658\r\n[2,] -0.030023304 -0.07454585 -0.003988336\r\n[3,]  0.197028288  0.28324096  0.348896805\r\n[4,]  0.002240307 -0.06560960 -0.044790680\r\n[5,]  0.022936731 -0.05256865 -0.063289287\r\n[6,]  0.101772051  0.04942613 -0.034439696\r\n\r\nSo, one can use these three new variables representing a linear,\r\nquadratic, and cubic trend in our prediction model instead of the\r\noriginal variable x. See below the relationship between the\r\noriginal variable x and the new polynomial features to replace\r\nit.\r\n\r\n\r\n\r\nFor continuous predictors, there is no limit for the degree of\r\npolynomial. The higher the degree of polynomial, the more flexible the\r\nmodel becomes, and there is a higher chance of overfitting. Typically,\r\npolynomial terms up to the 3rd or 4th degree are more than enough.\r\n5. Handling Missing Data\r\nMissing data deserves a course of its own. For a comprehensive review\r\nof how to handle and impute missing data. For certain types of models\r\nsuch as gradient boosting, missing data is not a problem, and one can\r\nleave them as is without any processing. On the other hand, some models\r\nsuch as regularized regression models require complete data and one have\r\nto deal with missing data before modeling data. It is always a good idea\r\nto run some simple descriptive analysis to understand the scope of\r\nmissing values in your dataset.\r\nThe ff_glimpse() function from the finalfit\r\npackage is a useful to get a quick look at the missing values in your\r\ndataset. See an example for the recidivism dataset\r\n\r\n\r\n\r\n\r\n\r\nrequire(finalfit)\r\n\r\nff_glimpse(recidivism)$Continuous[,c('n','missing_percent')]\r\n\r\n                                                      n\r\nID                                                18111\r\nResidence_PUMA                                    18111\r\nGang_Affiliated                                   15609\r\nSupervision_Risk_Score_First                      17819\r\nDependents                                        18111\r\nPrior_Arrest_Episodes_Felony                      18111\r\nPrior_Arrest_Episodes_Misd                        18111\r\nPrior_Arrest_Episodes_Violent                     18111\r\nPrior_Arrest_Episodes_Property                    18111\r\nPrior_Arrest_Episodes_Drug                        18111\r\nPrior_Arrest_Episodes_PPViolationCharges          18111\r\nPrior_Arrest_Episodes_DVCharges                   18111\r\nPrior_Arrest_Episodes_GunCharges                  18111\r\nPrior_Conviction_Episodes_Felony                  18111\r\nPrior_Conviction_Episodes_Misd                    18111\r\nPrior_Conviction_Episodes_Viol                    18111\r\nPrior_Conviction_Episodes_Prop                    18111\r\nPrior_Conviction_Episodes_Drug                    18111\r\nPrior_Conviction_Episodes_PPViolationCharges      18111\r\nPrior_Conviction_Episodes_DomesticViolenceCharges 18111\r\nPrior_Conviction_Episodes_GunCharges              18111\r\nPrior_Revocations_Parole                          18111\r\nPrior_Revocations_Probation                       18111\r\nCondition_MH_SA                                   18111\r\nCondition_Cog_Ed                                  18111\r\nCondition_Other                                   18111\r\nViolations_ElectronicMonitoring                   18111\r\nViolations_Instruction                            18111\r\nViolations_FailToReport                           18111\r\nViolations_MoveWithoutPermission                  18111\r\nDelinquency_Reports                               18111\r\nProgram_Attendances                               18111\r\nProgram_UnexcusedAbsences                         18111\r\nResidence_Changes                                 18111\r\nAvg_Days_per_DrugTest                             14343\r\nDrugTests_THC_Positive                            15254\r\nDrugTests_Cocaine_Positive                        15254\r\nDrugTests_Meth_Positive                           15254\r\nDrugTests_Other_Positive                          15254\r\nPercent_Days_Employed                             17649\r\nJobs_Per_Year                                     17303\r\nEmployment_Exempt                                 18111\r\nRecidivism_Within_3years                          18111\r\nRecidivism_Arrest_Year1                           18111\r\nRecidivism_Arrest_Year2                           18111\r\nRecidivism_Arrest_Year3                           18111\r\nTraining_Sample                                   18111\r\n                                                  missing_percent\r\nID                                                            0.0\r\nResidence_PUMA                                                0.0\r\nGang_Affiliated                                              13.8\r\nSupervision_Risk_Score_First                                  1.6\r\nDependents                                                    0.0\r\nPrior_Arrest_Episodes_Felony                                  0.0\r\nPrior_Arrest_Episodes_Misd                                    0.0\r\nPrior_Arrest_Episodes_Violent                                 0.0\r\nPrior_Arrest_Episodes_Property                                0.0\r\nPrior_Arrest_Episodes_Drug                                    0.0\r\nPrior_Arrest_Episodes_PPViolationCharges                      0.0\r\nPrior_Arrest_Episodes_DVCharges                               0.0\r\nPrior_Arrest_Episodes_GunCharges                              0.0\r\nPrior_Conviction_Episodes_Felony                              0.0\r\nPrior_Conviction_Episodes_Misd                                0.0\r\nPrior_Conviction_Episodes_Viol                                0.0\r\nPrior_Conviction_Episodes_Prop                                0.0\r\nPrior_Conviction_Episodes_Drug                                0.0\r\nPrior_Conviction_Episodes_PPViolationCharges                  0.0\r\nPrior_Conviction_Episodes_DomesticViolenceCharges             0.0\r\nPrior_Conviction_Episodes_GunCharges                          0.0\r\nPrior_Revocations_Parole                                      0.0\r\nPrior_Revocations_Probation                                   0.0\r\nCondition_MH_SA                                               0.0\r\nCondition_Cog_Ed                                              0.0\r\nCondition_Other                                               0.0\r\nViolations_ElectronicMonitoring                               0.0\r\nViolations_Instruction                                        0.0\r\nViolations_FailToReport                                       0.0\r\nViolations_MoveWithoutPermission                              0.0\r\nDelinquency_Reports                                           0.0\r\nProgram_Attendances                                           0.0\r\nProgram_UnexcusedAbsences                                     0.0\r\nResidence_Changes                                             0.0\r\nAvg_Days_per_DrugTest                                        20.8\r\nDrugTests_THC_Positive                                       15.8\r\nDrugTests_Cocaine_Positive                                   15.8\r\nDrugTests_Meth_Positive                                      15.8\r\nDrugTests_Other_Positive                                     15.8\r\nPercent_Days_Employed                                         2.6\r\nJobs_Per_Year                                                 4.5\r\nEmployment_Exempt                                             0.0\r\nRecidivism_Within_3years                                      0.0\r\nRecidivism_Arrest_Year1                                       0.0\r\nRecidivism_Arrest_Year2                                       0.0\r\nRecidivism_Arrest_Year3                                       0.0\r\nTraining_Sample                                               0.0\r\n\r\nff_glimpse(recidivism)$Categorical[,c('n','missing_percent')]\r\n\r\n                            n missing_percent\r\nGender                  18111             0.0\r\nRace                    18111             0.0\r\nAge_at_Release          18111             0.0\r\nSupervision_Level_First 16962             6.3\r\nEducation_Level         18111             0.0\r\nPrison_Offense          15820            12.6\r\nPrison_Years            18111             0.0\r\n\r\nWe will focus a few ideas about how to handle missing data.\r\n5.1. Creating an\r\nindicator variable for missingness\r\nWe can create a binary indicator variable for every variable to\r\nindicate missingness (0: not missing, 1: missing). It doesn’t solve the\r\nmissing data problem because we may still have to impute the missing\r\nvalues for modeling; however, an indicator variable about whether or not\r\na variable is missing may sometimes provide some information in\r\npredicting the outcome. Suppose the missingness is not random, and there\r\nis a systematic relationship between outcome and whether or not values\r\nare missing for a variable. In that case, this may provide vital\r\ninformation to bring into the model. This indicator variable would be\r\nmeaningless for variables that don’t have any missing value. Therefore,\r\none can remove them from any further consideration.\r\n5.2. Imputation\r\nA common approach to missing data is to impute missing values. Each\r\npredictor becomes an outcome of interest in imputation, and then the\r\nremaining predictors are used to build an imputation model to predict\r\nthe missing values. Below is a very naive example of how it would work\r\nif we have an outcome variable (Y) and three predictors (X1, X2, X3).\r\nFirst, missing values are estimated and replaced for each predictor\r\nusing an imputation model, and then the primary outcome of interest is\r\npredicted using the imputed X1, X2, and X3.\r\nImputation Model\r\n\r\nOutcome\r\nPredictors\r\nX1\r\nX2,X3\r\nX2\r\nX1,X3\r\nX3\r\nX1,X2\r\nPrediction Model\r\n\r\nOutcome\r\nPredictors\r\nY\r\nX1, X2, X3\r\nAn imputation model can be as simple as an intercept-only model (mean\r\nimputation). For numeric variables, missing values can be replaced with\r\na simple mean, median, or mode of the observed data. For categorical\r\nvariables, missing values can be replaced with a value randomly drawn\r\nfrom a binomial or multinomial distribution with the observed\r\nprobabilities.\r\nAn imputation model can also be as complex as desired using a\r\nregularized regression model, a decision tree model, or a K-nearest\r\nneighbors model. The main idea of a more complex prediction model is to\r\nfind other observations similar to observations with a missing value in\r\nterms of other predictors and use data from these similar observations\r\nto predict the missing values. We will rely on some built-in functions\r\nin R to impute values using such complex models. For more information\r\nabout its theoretical foundations, Applied Missing Data Analysis\r\nby Craig Enders provides comprehensive coverage of this topic.\r\n6. Wrapping-up using the\r\nrecipes package\r\nWe can manually pre-process all the variables in the dataset using\r\nthe approaches discussed earlier. However, this would be a tedious job.\r\nIt may be overwhelming to apply all the procedures simultaneously to\r\ndifferent versions of the datasets (e.g., training, test, future).\r\nInstead, we can use the recipes package to implement these\r\napproaches in a more organized and efficient way.\r\nBefore using the recipes package, there are a few things\r\nto do for this dataset.\r\nRead the original data\r\nMake a list of variables with different characteristics\r\n(categorical, continuous, proportions, etc.)\r\nMake sure the type of all categorical variables is either\r\ncharacter or factor.\r\nFor variables that represent proportions, add/subtract a small\r\nnumber to 0s/1s for logit transformation\r\n\r\n\r\n# 1) Read the original data\r\n\r\n  recidivism <- read.csv(here('data/recidivism_y1 removed and recoded.csv'),header=TRUE)\r\n\r\n  str(recidivism)\r\n\r\n'data.frame':   18111 obs. of  54 variables:\r\n $ ID                                               : int  1 2 3 4 6 7 8 11 13 15 ...\r\n $ Gender                                           : chr  \"M\" \"M\" \"M\" \"M\" ...\r\n $ Race                                             : chr  \"BLACK\" \"BLACK\" \"BLACK\" \"WHITE\" ...\r\n $ Age_at_Release                                   : chr  \"43-47\" \"33-37\" \"48 or older\" \"38-42\" ...\r\n $ Residence_PUMA                                   : int  16 16 24 16 17 18 16 5 18 5 ...\r\n $ Gang_Affiliated                                  : int  0 0 0 0 0 0 0 0 0 0 ...\r\n $ Supervision_Risk_Score_First                     : int  3 6 7 7 5 2 5 3 3 7 ...\r\n $ Supervision_Level_First                          : chr  \"Standard\" \"Specialized\" \"High\" \"High\" ...\r\n $ Education_Level                                  : chr  \"At least some college\" \"Less than HS diploma\" \"At least some college\" \"Less than HS diploma\" ...\r\n $ Dependents                                       : int  3 1 3 1 0 2 3 1 1 1 ...\r\n $ Prison_Offense                                   : chr  \"Drug\" \"Violent/Non-Sex\" \"Drug\" \"Property\" ...\r\n $ Prison_Years                                     : chr  \"More than 3 years\" \"More than 3 years\" \"1-2 years\" \"1-2 years\" ...\r\n $ Prior_Arrest_Episodes_Felony                     : int  6 7 6 8 4 10 6 3 8 9 ...\r\n $ Prior_Arrest_Episodes_Misd                       : int  6 6 6 6 0 6 6 6 4 3 ...\r\n $ Prior_Arrest_Episodes_Violent                    : int  1 3 3 0 1 1 3 2 0 2 ...\r\n $ Prior_Arrest_Episodes_Property                   : int  3 0 2 3 3 5 1 1 5 2 ...\r\n $ Prior_Arrest_Episodes_Drug                       : int  3 3 2 3 0 1 2 1 2 4 ...\r\n $ Prior_Arrest_Episodes_PPViolationCharges         : int  4 5 5 3 0 5 5 3 1 4 ...\r\n $ Prior_Arrest_Episodes_DVCharges                  : int  0 1 1 0 0 0 0 1 0 0 ...\r\n $ Prior_Arrest_Episodes_GunCharges                 : int  0 0 0 0 0 1 0 0 0 1 ...\r\n $ Prior_Conviction_Episodes_Felony                 : int  3 3 3 3 1 3 1 0 1 3 ...\r\n $ Prior_Conviction_Episodes_Misd                   : int  3 4 2 4 0 1 4 3 0 2 ...\r\n $ Prior_Conviction_Episodes_Viol                   : int  0 1 1 0 0 0 1 0 0 1 ...\r\n $ Prior_Conviction_Episodes_Prop                   : int  2 0 1 3 2 3 0 0 2 1 ...\r\n $ Prior_Conviction_Episodes_Drug                   : int  2 2 2 2 0 0 2 0 1 1 ...\r\n $ Prior_Conviction_Episodes_PPViolationCharges     : int  0 1 0 0 0 1 1 1 0 1 ...\r\n $ Prior_Conviction_Episodes_DomesticViolenceCharges: int  0 1 1 0 0 0 0 0 0 0 ...\r\n $ Prior_Conviction_Episodes_GunCharges             : int  0 1 0 0 0 1 0 0 0 0 ...\r\n $ Prior_Revocations_Parole                         : int  0 0 0 0 0 0 0 1 0 0 ...\r\n $ Prior_Revocations_Probation                      : int  0 0 0 1 0 0 0 0 0 0 ...\r\n $ Condition_MH_SA                                  : int  1 0 1 1 0 0 0 1 0 1 ...\r\n $ Condition_Cog_Ed                                 : int  1 0 1 1 0 0 1 1 0 1 ...\r\n $ Condition_Other                                  : int  0 0 0 0 1 0 0 0 0 1 ...\r\n $ Violations_ElectronicMonitoring                  : int  0 0 0 0 0 0 0 1 0 0 ...\r\n $ Violations_Instruction                           : int  0 1 1 0 0 0 0 1 0 0 ...\r\n $ Violations_FailToReport                          : int  0 0 0 0 0 0 0 0 0 0 ...\r\n $ Violations_MoveWithoutPermission                 : int  0 0 1 0 1 0 0 0 0 0 ...\r\n $ Delinquency_Reports                              : int  0 4 4 0 0 0 0 0 0 0 ...\r\n $ Program_Attendances                              : int  6 0 6 6 0 0 0 9 0 6 ...\r\n $ Program_UnexcusedAbsences                        : int  0 0 0 0 0 0 0 2 0 0 ...\r\n $ Residence_Changes                                : int  2 2 0 3 3 1 0 2 1 1 ...\r\n $ Avg_Days_per_DrugTest                            : num  612 35.7 93.7 25.4 474.6 ...\r\n $ DrugTests_THC_Positive                           : num  0 0 0.333 0 0 ...\r\n $ DrugTests_Cocaine_Positive                       : num  0 0 0 0 0 0 0 0 0 0 ...\r\n $ DrugTests_Meth_Positive                          : num  0 0 0.167 0 0 ...\r\n $ DrugTests_Other_Positive                         : num  0 0 0 0 0 ...\r\n $ Percent_Days_Employed                            : num  0.489 0.425 0 1 0.674 ...\r\n $ Jobs_Per_Year                                    : num  0.448 2 0 0.719 0.308 ...\r\n $ Employment_Exempt                                : int  0 0 0 0 0 0 0 1 0 1 ...\r\n $ Recidivism_Within_3years                         : int  0 1 1 0 0 1 0 1 0 0 ...\r\n $ Recidivism_Arrest_Year1                          : int  0 0 0 0 0 0 0 0 0 0 ...\r\n $ Recidivism_Arrest_Year2                          : int  0 0 1 0 0 0 0 1 0 0 ...\r\n $ Recidivism_Arrest_Year3                          : int  0 1 0 0 0 1 0 0 0 0 ...\r\n $ Training_Sample                                  : int  1 1 1 1 0 1 0 1 1 0 ...\r\n\r\n# 2) List of variable types\r\n  \r\n  outcome <- c('Recidivism_Arrest_Year2')\r\n  \r\n  id      <- c('ID')\r\n  \r\n  categorical <- c('Residence_PUMA',\r\n                   'Prison_Offense',\r\n                   'Age_at_Release',\r\n                   'Supervision_Level_First',\r\n                   'Education_Level',\r\n                   'Prison_Years',\r\n                   'Gender',\r\n                   'Race',\r\n                   'Gang_Affiliated',\r\n                   'Prior_Arrest_Episodes_DVCharges',\r\n                   'Prior_Arrest_Episodes_GunCharges',\r\n                   'Prior_Conviction_Episodes_Viol',\r\n                   'Prior_Conviction_Episodes_PPViolationCharges',\r\n                   'Prior_Conviction_Episodes_DomesticViolenceCharges',\r\n                   'Prior_Conviction_Episodes_GunCharges',\r\n                   'Prior_Revocations_Parole',\r\n                   'Prior_Revocations_Probation',\r\n                   'Condition_MH_SA',\r\n                   'Condition_Cog_Ed',\r\n                   'Condition_Other',\r\n                   'Violations_ElectronicMonitoring',\r\n                   'Violations_Instruction',\r\n                   'Violations_FailToReport',\r\n                   'Violations_MoveWithoutPermission',\r\n                   'Employment_Exempt') \r\n\r\n  numeric   <- c('Supervision_Risk_Score_First',\r\n                 'Dependents',\r\n                 'Prior_Arrest_Episodes_Felony',\r\n                 'Prior_Arrest_Episodes_Misd',\r\n                 'Prior_Arrest_Episodes_Violent',\r\n                 'Prior_Arrest_Episodes_Property',\r\n                 'Prior_Arrest_Episodes_Drug',\r\n                 'Prior_Arrest_Episodes_PPViolationCharges',\r\n                 'Prior_Conviction_Episodes_Felony',\r\n                 'Prior_Conviction_Episodes_Misd',\r\n                 'Prior_Conviction_Episodes_Prop',\r\n                 'Prior_Conviction_Episodes_Drug',\r\n                 'Delinquency_Reports',\r\n                 'Program_Attendances',\r\n                 'Program_UnexcusedAbsences',\r\n                 'Residence_Changes',\r\n                 'Avg_Days_per_DrugTest',\r\n                 'Jobs_Per_Year')\r\n  \r\n  props      <- c('DrugTests_THC_Positive',\r\n                  'DrugTests_Cocaine_Positive',\r\n                  'DrugTests_Meth_Positive',\r\n                  'DrugTests_Other_Positive',\r\n                  'Percent_Days_Employed')\r\n  \r\n# 3) Convert all nominal, ordinal, and binary variables to factors\r\n  # Leave the rest as is\r\n  \r\n  for(i in categorical){\r\n    \r\n    recidivism[,i] <- as.factor(recidivism[,i])\r\n    \r\n  }\r\n  \r\n# 4) For variables that represent proportions, add/substract a small number\r\n  # to 0s/1s for logit transformation\r\n  \r\n  for(i in props){\r\n    recidivism[,i] <- ifelse(recidivism[,i]==0,.0001,recidivism[,i])\r\n    recidivism[,i] <- ifelse(recidivism[,i]==1,.9999,recidivism[,i])\r\n  }\r\n\r\n\r\nNow, we can apply certain transformations to different types of\r\nvariables. We will use the step_***() functions from the\r\nrecipes package to implement different procedures. Below is\r\na list of functions for most procedures discussed earlier in this\r\nlecture. A more detailed list of step_***() functions can\r\nbe found the in the\r\npackage manual.\r\nstep_dummy(): creates dummy variables for one-hot\r\nencoding of categorical variables\r\nstep_indicate_na() creates an indicator variable for\r\nmissingness\r\nstep_impute_bag(), step_impute_knn(),\r\nstep_impute_linear(), and step_impute_mean():\r\nimputes missing values using an imputation model\r\nstep_BoxCox(): transforms non-negative data using\r\nBox-Cox method\r\nstep_poly,step_bs(), and\r\nstep_ns() : creates basis expansions\r\nstep_logit: applies logit transformation\r\nstep_zv: removes variables with zero variance\r\nstep_normalize: standardize variables to have a mean of\r\nzero and standard deviation of one\r\nNote that the order of procedures applied to variables is important.\r\nFor instance, there would be no meaning of using\r\nstep_indicate_na() after using\r\nstep_impute_bag() (Why?). Or, there will be a problem when\r\nyou first standardize variables using ’step_normalize()` and then apply\r\na Box-Cox transformation (Why?).\r\nFor this dataset, we will implement the following steps:\r\nCreate an indicator variable of missingness for all predictors\r\n(step_indicate_na)\r\nRemove the variables with zero variance (step_zv)\r\nImpute the missing values for all predictor variables using a mean\r\nor mode (step_impute_mean and\r\nstep_impute_mode)\r\nLogit transform the variables that represent\r\nproportions(step_logit)\r\nCreate polynomial terms up to the 2nf degree term for all numeric\r\nvariables (step_poly)\r\nStandardize numeric features\r\nOne-hot encoding of all categorical variables\r\n(step_dummy)\r\n\r\n\r\nrequire(recipes)\r\n\r\nblueprint <- recipe(x  = recidivism,\r\n                    vars  = c(categorical,numeric,props,outcome,id),\r\n                    roles = c(rep('predictor',48),'outcome','ID')) %>%\r\n  \r\n  # for all 48 predictors, create an indicator variable for missingness\r\n\r\n  step_indicate_na(all_of(categorical),all_of(numeric),all_of(props)) %>%\r\n  \r\n  # Remove the variable with zero variance, this will also remove the missingness \r\n  # variables if there is no missingess\r\n\r\n  step_zv(all_numeric()) %>%\r\n  \r\n  # Impute the missing values using mean and mode. You can instead use a \r\n  # more advanced imputation model such as bagged trees. I haven't used it due\r\n  # to time concerns\r\n  \r\n  step_impute_mean(all_of(numeric),all_of(props)) %>%\r\n  step_impute_mode(all_of(categorical)) %>%\r\n  \r\n  #Logit transformation of proportions\r\n  \r\n  step_logit(all_of(props)) %>%\r\n  \r\n  # Natural splines for numeric variables and proportions\r\n  \r\n  step_poly(all_of(numeric),all_of(props),degree=2) %>%\r\n  \r\n  # Standardize the natural splines of numeric variables and proportions\r\n  \r\n  step_normalize(paste0(numeric,'_poly_1'),\r\n                 paste0(numeric,'_poly_2'),\r\n                 paste0(props,'_poly_1'),\r\n                 paste0(props,'_poly_2')) %>%\r\n  \r\n  # One-hot encoding for all categorical variables\r\n  \r\n  step_dummy(all_of(categorical),one_hot=TRUE)\r\n\r\n\r\n\r\nblueprint\r\n\r\nRecipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n        ID          1\r\n   outcome          1\r\n predictor         48\r\n\r\nOperations:\r\n\r\nCreating missing data variable indicators for all_of(categorical), all_of(numeric),...\r\nZero variance filter on all_numeric()\r\nMean imputation for all_of(numeric), all_of(props)\r\nMode imputation for all_of(categorical)\r\nLogit transformation on all_of(props)\r\nOrthogonal polynomials on all_of(numeric), all_of(props)\r\nCentering and scaling for paste0(numeric, \"_poly_1\"), paste0(nu...\r\nDummy variables from all_of(categorical)\r\n\r\nOnce the recipe is ready, we can train the blueprint on training\r\ndata. When we say ‘train’, it means that weights or statistics for\r\ncertain types of operations (e.g., standardization) are calculated based\r\non training data and saved for later use. For instance, the mean and\r\nstandard deviation of variable X from training data is calculated and\r\nlater used to standardize the same variable X in testing data or future\r\ndatasets. For now, we will train the blueprint using the whole dataset.\r\nIn future lectures, we will split data into different subsets (training\r\nvs. test), and use the training dataset to apply the blueprint.\r\n\r\n\r\nprepare <- prep(blueprint, \r\n                training = recidivism)\r\nprepare\r\n\r\nRecipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n        ID          1\r\n   outcome          1\r\n predictor         48\r\n\r\nTraining data contained 18111 data points and 7977 incomplete rows. \r\n\r\nOperations:\r\n\r\nCreating missing data variable indicators for Residence_PUMA, Prison_Offense, Age_a... [trained]\r\nZero variance filter removed na_ind_Residence_PUMA, na_ind... [trained]\r\nMean imputation for Supervision_Risk_Score_First, Depende... [trained]\r\nMode imputation for Residence_PUMA, Prison_Offense, Age_a... [trained]\r\nLogit transformation on DrugTests_THC_Positive, DrugTests_... [trained]\r\nOrthogonal polynomials on Supervision_Risk_Score_First, De... [trained]\r\nCentering and scaling for Supervision_Risk_Score_First_poly_1, ... [trained]\r\nDummy variables from Residence_PUMA, Prison_Offense, Age_at_Release,... [trained]\r\n\r\nFinally, we can apply this recipe to our dataset to obtain processed\r\nvariables according to the recipe.\r\n\r\n\r\nbaked_recidivism <- bake(prepare, new_data = recidivism)\r\n\r\nbaked_recidivism\r\n\r\n# A tibble: 18,111 x 144\r\n   Recidivism_Arrest_Year2    ID na_ind_Prison_Offen~ na_ind_Supervis~\r\n                     <int> <int>                <int>            <int>\r\n 1                       0     1                    0                0\r\n 2                       0     2                    0                0\r\n 3                       1     3                    0                0\r\n 4                       0     4                    0                0\r\n 5                       0     6                    0                0\r\n 6                       0     7                    1                0\r\n 7                       0     8                    0                0\r\n 8                       1    11                    0                0\r\n 9                       0    13                    0                0\r\n10                       0    15                    0                0\r\n# ... with 18,101 more rows, and 140 more variables:\r\n#   na_ind_Gang_Affiliated <int>,\r\n#   na_ind_Supervision_Risk_Score_First <int>,\r\n#   na_ind_Avg_Days_per_DrugTest <int>, na_ind_Jobs_Per_Year <int>,\r\n#   na_ind_DrugTests_THC_Positive <int>,\r\n#   na_ind_DrugTests_Cocaine_Positive <int>,\r\n#   na_ind_DrugTests_Meth_Positive <int>, ...\r\n\r\nNotice that there are 144 variables (one outcome, one ID, and 142\r\npredictors) in the new processed dataset. In the original dataset, there\r\nwere 48 predictors. Below is a breakdown of where these 142 variables\r\ncome from.\r\nVariable Name\r\nEncoding\r\nNumber of Categories\r\nProcess\r\nNumber of Constructed Variables\r\nGender\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nRace\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nGang affiliation\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Arrest_Episodes_DVCharges\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Arrest_Episodes_GunCharges\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Conviction_Episodes_Viol\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Conviction_Episodes_PPViolationCharges\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Conviction_Episodes_DomesticViolenceCharges\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Conviction_Episodes_GunCharges\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Revocations_Parole\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Revocations_Probation\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nCondition_MH_SA\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nCondition_Cog_Ed\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nCondition_Other\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nViolations_ElectronicMonitoring\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nViolations_Instruction\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nViolations_FailToReport\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nViolations_MoveWithoutPermission\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nEmployment_Exempt\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nAge_at_Release\r\nOrdinal\r\n7\r\nOne-hot encoding\r\n7\r\nSupervision_Level_First\r\nOrdinal\r\n3\r\nOne-hot encoding\r\n3\r\nEducation_Level\r\nOrdinal\r\n3\r\nOne-hot encoding\r\n3\r\nPrison_Years\r\nOrdinal\r\n4\r\nOne-hot encoding\r\n4\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nResidence_PUMA\r\nNominal\r\n25\r\nOne-hot encoding\r\n25\r\nPrison_Offense\r\nNominal\r\n5\r\nOne-hot encoding\r\n5\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSupervision_Risk_Score_First\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nDependents\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Arrest_Episodes_Felony\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Arrest_Episodes_Misd\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Arrest_Episodes_Violent\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Arrest_Episodes_Property\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Arrest_Episodes_Drug\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Arrest_Episodes_PPViolationCharges\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Conviction_Episodes_Felony\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Conviction_Episodes_Misd\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Conviction_Episodes_Prop\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Conviction_Episodes_Drug\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nDelinquency_Reports\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nProgram_Attendances\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nProgram_UnexcusedAbsences\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nResidence_Changes\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nAvg_Days_per_DrugTest\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nJobs_Per_Year\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nDrugTests_THC_Positive\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nDrugTests_Cocaine_Positive\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nDrugTests_Meth_Positive\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nDrugTests_Other_Positive\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPercent_Days_Employed\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nMissing value indicator variables\r\nBinary\r\n\r\n\r\n11\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nTotal\r\n142\r\n\r\n\r\n\r\n",
      "last_modified": "2022-07-06T19:21:35-07:00"
    },
    {
      "path": "LICENSE.html",
      "author": [],
      "contents": "\r\nAttribution 4.0 International\r\n=======================================================================\r\nCreative Commons Corporation (“Creative Commons”) is not a law firm\r\nand does not provide legal services or legal advice. Distribution of\r\nCreative Commons public licenses does not create a lawyer-client or\r\nother relationship. Creative Commons makes its licenses and related\r\ninformation available on an “as-is” basis. Creative Commons gives no\r\nwarranties regarding its licenses, any material licensed under their\r\nterms and conditions, or any related information. Creative Commons\r\ndisclaims all liability for damages resulting from their use to the\r\nfullest extent possible.\r\nUsing Creative Commons Public Licenses\r\nCreative Commons public licenses provide a standard set of terms and\r\nconditions that creators and other rights holders may use to share\r\noriginal works of authorship and other material subject to copyright and\r\ncertain other rights specified in the public license below. The\r\nfollowing considerations are for informational purposes only, are not\r\nexhaustive, and do not form part of our licenses.\r\n Considerations for licensors: Our public licenses are\r\n intended for use by those authorized to give the public\r\n permission to use material in ways otherwise restricted by\r\n copyright and certain other rights. Our licenses are\r\n irrevocable. Licensors should read and understand the terms\r\n and conditions of the license they choose before applying it.\r\n Licensors should also secure all rights necessary before\r\n applying our licenses so that the public can reuse the\r\n material as expected. Licensors should clearly mark any\r\n material not subject to the license. This includes other CC-\r\n licensed material, or material used under an exception or\r\n limitation to copyright. More considerations for licensors:\r\nwiki.creativecommons.org/Considerations_for_licensors\r\n\r\n Considerations for the public: By using one of our public\r\n licenses, a licensor grants the public permission to use the\r\n licensed material under specified terms and conditions. If\r\n the licensor's permission is not necessary for any reason--for\r\n example, because of any applicable exception or limitation to\r\n copyright--then that use is not regulated by the license. Our\r\n licenses grant only permissions under copyright and certain\r\n other rights that a licensor has authority to grant. Use of\r\n the licensed material may still be restricted for other\r\n reasons, including because others have copyright or other\r\n rights in the material. A licensor may make special requests,\r\n such as asking that all changes be marked or described.\r\n Although not required by our licenses, you are encouraged to\r\n respect those requests where reasonable. More considerations\r\n for the public: \r\nwiki.creativecommons.org/Considerations_for_licensees\r\n=======================================================================\r\nCreative Commons Attribution 4.0 International Public License\r\nBy exercising the Licensed Rights (defined below), You accept and\r\nagree to be bound by the terms and conditions of this Creative Commons\r\nAttribution 4.0 International Public License (“Public License”). To the\r\nextent this Public License may be interpreted as a contract, You are\r\ngranted the Licensed Rights in consideration of Your acceptance of these\r\nterms and conditions, and the Licensor grants You such rights in\r\nconsideration of benefits the Licensor receives from making the Licensed\r\nMaterial available under these terms and conditions.\r\nSection 1 – Definitions.\r\nAdapted Material means material subject to Copyright and Similar\r\nRights that is derived from or based upon the Licensed Material and in\r\nwhich the Licensed Material is translated, altered, arranged,\r\ntransformed, or otherwise modified in a manner requiring permission\r\nunder the Copyright and Similar Rights held by the Licensor. For\r\npurposes of this Public License, where the Licensed Material is a\r\nmusical work, performance, or sound recording, Adapted Material is\r\nalways produced where the Licensed Material is synched in timed relation\r\nwith a moving image.\r\nAdapter’s License means the license You apply to Your Copyright\r\nand Similar Rights in Your contributions to Adapted Material in\r\naccordance with the terms and conditions of this Public\r\nLicense.\r\nCopyright and Similar Rights means copyright and/or similar\r\nrights closely related to copyright including, without limitation,\r\nperformance, broadcast, sound recording, and Sui Generis Database\r\nRights, without regard to how the rights are labeled or categorized. For\r\npurposes of this Public License, the rights specified in Section\r\n2(b)(1)-(2) are not Copyright and Similar Rights.\r\nEffective Technological Measures means those measures that, in\r\nthe absence of proper authority, may not be circumvented under laws\r\nfulfilling obligations under Article 11 of the WIPO Copyright Treaty\r\nadopted on December 20, 1996, and/or similar international\r\nagreements.\r\nExceptions and Limitations means fair use, fair dealing, and/or\r\nany other exception or limitation to Copyright and Similar Rights that\r\napplies to Your use of the Licensed Material.\r\nLicensed Material means the artistic or literary work, database,\r\nor other material to which the Licensor applied this Public\r\nLicense.\r\nLicensed Rights means the rights granted to You subject to the\r\nterms and conditions of this Public License, which are limited to all\r\nCopyright and Similar Rights that apply to Your use of the Licensed\r\nMaterial and that the Licensor has authority to license.\r\nLicensor means the individual(s) or entity(ies) granting rights\r\nunder this Public License.\r\nShare means to provide material to the public by any means or\r\nprocess that requires permission under the Licensed Rights, such as\r\nreproduction, public display, public performance, distribution,\r\ndissemination, communication, or importation, and to make material\r\navailable to the public including in ways that members of the public may\r\naccess the material from a place and at a time individually chosen by\r\nthem.\r\nSui Generis Database Rights means rights other than copyright\r\nresulting from Directive 96/9/EC of the European Parliament and of the\r\nCouncil of 11 March 1996 on the legal protection of databases, as\r\namended and/or succeeded, as well as other essentially equivalent rights\r\nanywhere in the world.\r\nYou means the individual or entity exercising the Licensed Rights\r\nunder this Public License. Your has a corresponding meaning.\r\nSection 2 – Scope.\r\nLicense grant.\r\nSubject to the terms and conditions of this Public License, the\r\nLicensor hereby grants You a worldwide, royalty-free, non-sublicensable,\r\nnon-exclusive, irrevocable license to exercise the Licensed Rights in\r\nthe Licensed Material to:\r\nreproduce and Share the Licensed Material, in whole or in part;\r\nand\r\nproduce, reproduce, and Share Adapted Material.\r\n\r\nExceptions and Limitations. For the avoidance of doubt, where\r\nExceptions and Limitations apply to Your use, this Public License does\r\nnot apply, and You do not need to comply with its terms and\r\nconditions.\r\nTerm. The term of this Public License is specified in Section\r\n6(a).\r\nMedia and formats; technical modifications allowed. The Licensor\r\nauthorizes You to exercise the Licensed Rights in all media and formats\r\nwhether now known or hereafter created, and to make technical\r\nmodifications necessary to do so. The Licensor waives and/or agrees not\r\nto assert any right or authority to forbid You from making technical\r\nmodifications necessary to exercise the Licensed Rights, including\r\ntechnical modifications necessary to circumvent Effective Technological\r\nMeasures. For purposes of this Public License, simply making\r\nmodifications authorized by this Section 2(a)\r\nnever produces Adapted Material.\r\n\r\nDownstream recipients.\r\nOffer from the Licensor – Licensed Material. Every recipient of\r\nthe Licensed Material automatically receives an offer from the Licensor\r\nto exercise the Licensed Rights under the terms and conditions of this\r\nPublic License.\r\nNo downstream restrictions. You may not offer or impose any\r\nadditional or different terms or conditions on, or apply any Effective\r\nTechnological Measures to, the Licensed Material if doing so restricts\r\nexercise of the Licensed Rights by any recipient of the Licensed\r\nMaterial.\r\n\r\nNo endorsement. Nothing in this Public License constitutes or may\r\nbe construed as permission to assert or imply that You are, or that Your\r\nuse of the Licensed Material is, connected with, or sponsored, endorsed,\r\nor granted official status by, the Licensor or others designated to\r\nreceive attribution as provided in Section 3(a)(1)(A)(i).\r\n\r\nOther rights.\r\nMoral rights, such as the right of integrity, are not licensed\r\nunder this Public License, nor are publicity, privacy, and/or other\r\nsimilar personality rights; however, to the extent possible, the\r\nLicensor waives and/or agrees not to assert any such rights held by the\r\nLicensor to the limited extent necessary to allow You to exercise the\r\nLicensed Rights, but not otherwise.\r\nPatent and trademark rights are not licensed under this Public\r\nLicense.\r\nTo the extent possible, the Licensor waives any right to collect\r\nroyalties from You for the exercise of the Licensed Rights, whether\r\ndirectly or through a collecting society under any voluntary or waivable\r\nstatutory or compulsory licensing scheme. In all other cases the\r\nLicensor expressly reserves any right to collect such\r\nroyalties.\r\n\r\nSection 3 – License Conditions.\r\nYour exercise of the Licensed Rights is expressly made subject to the\r\nfollowing conditions.\r\nAttribution.\r\nIf You Share the Licensed Material (including in modified form),\r\nYou must:\r\nretain the following if it is supplied by the Licensor with the\r\nLicensed Material:\r\nidentification of the creator(s) of the Licensed Material and any\r\nothers designated to receive attribution, in any reasonable manner\r\nrequested by the Licensor (including by pseudonym if\r\ndesignated);\r\na copyright notice;\r\na notice that refers to this Public License;\r\na notice that refers to the disclaimer of warranties;\r\na URI or hyperlink to the Licensed Material to the extent\r\nreasonably practicable;\r\n\r\nindicate if You modified the Licensed Material and retain an\r\nindication of any previous modifications; and\r\nindicate the Licensed Material is licensed under this Public\r\nLicense, and include the text of, or the URI or hyperlink to, this\r\nPublic License.\r\n\r\nYou may satisfy the conditions in Section 3(a)(1) in any\r\nreasonable manner based on the medium, means, and context in which You\r\nShare the Licensed Material. For example, it may be reasonable to\r\nsatisfy the conditions by providing a URI or hyperlink to a resource\r\nthat includes the required information.\r\nIf requested by the Licensor, You must remove any of the\r\ninformation required by Section 3(a)(1)(A) to the extent reasonably\r\npracticable.\r\nIf You Share Adapted Material You produce, the Adapter’s License\r\nYou apply must not prevent recipients of the Adapted Material from\r\ncomplying with this Public License.\r\n\r\nSection 4 – Sui Generis Database Rights.\r\nWhere the Licensed Rights include Sui Generis Database Rights that\r\napply to Your use of the Licensed Material:\r\nfor the avoidance of doubt, Section 2(a)(1) grants You the right\r\nto extract, reuse, reproduce, and Share all or a substantial portion of\r\nthe contents of the database;\r\nif You include all or a substantial portion of the database\r\ncontents in a database in which You have Sui Generis Database Rights,\r\nthen the database in which You have Sui Generis Database Rights (but not\r\nits individual contents) is Adapted Material; and\r\nYou must comply with the conditions in Section 3(a) if You Share\r\nall or a substantial portion of the contents of the database.\r\nFor the avoidance of doubt, this Section 4 supplements and does not\r\nreplace Your obligations under this Public License where the Licensed\r\nRights include other Copyright and Similar Rights.\r\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\r\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE\r\nEXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND\r\nAS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND\r\nCONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY,\r\nOR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE,\r\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT,\r\nABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE\r\nOF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF\r\nWARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT\r\nAPPLY TO YOU.\r\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE\r\nTO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE)\r\nOR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL,\r\nCONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR\r\nDAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED\r\nMATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF\r\nSUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF\r\nLIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT\r\nAPPLY TO YOU.\r\nThe disclaimer of warranties and limitation of liability provided\r\nabove shall be interpreted in a manner that, to the extent possible,\r\nmost closely approximates an absolute disclaimer and waiver of all\r\nliability.\r\nSection 6 – Term and Termination.\r\nThis Public License applies for the term of the Copyright and\r\nSimilar Rights licensed here. However, if You fail to comply with this\r\nPublic License, then Your rights under this Public License terminate\r\nautomatically.\r\nWhere Your right to use the Licensed Material has terminated\r\nunder Section 6(a), it reinstates:\r\nautomatically as of the date the violation is cured, provided it\r\nis cured within 30 days of Your discovery of the violation; or\r\nupon express reinstatement by the Licensor.\r\nFor the avoidance of doubt, this Section 6(b) does not affect any\r\nright the Licensor may have to seek remedies for Your violations of this\r\nPublic License.\r\nFor the avoidance of doubt, the Licensor may also offer the\r\nLicensed Material under separate terms or conditions or stop\r\ndistributing the Licensed Material at any time; however, doing so will\r\nnot terminate this Public License.\r\nSections 1, 5, 6, 7, and 8 survive termination of this Public\r\nLicense.\r\nSection 7 – Other Terms and Conditions.\r\nThe Licensor shall not be bound by any additional or different\r\nterms or conditions communicated by You unless expressly\r\nagreed.\r\nAny arrangements, understandings, or agreements regarding the\r\nLicensed Material not stated herein are separate from and independent of\r\nthe terms and conditions of this Public License.\r\nSection 8 – Interpretation.\r\nFor the avoidance of doubt, this Public License does not, and\r\nshall not be interpreted to, reduce, limit, restrict, or impose\r\nconditions on any use of the Licensed Material that could lawfully be\r\nmade without permission under this Public License.\r\nTo the extent possible, if any provision of this Public License\r\nis deemed unenforceable, it shall be automatically reformed to the\r\nminimum extent necessary to make it enforceable. If the provision cannot\r\nbe reformed, it shall be severed from this Public License without\r\naffecting the enforceability of the remaining terms and\r\nconditions.\r\nNo term or condition of this Public License will be waived and no\r\nfailure to comply consented to unless expressly agreed to by the\r\nLicensor.\r\nNothing in this Public License constitutes or may be interpreted\r\nas a limitation upon, or waiver of, any privileges and immunities that\r\napply to the Licensor or You, including from the legal processes of any\r\njurisdiction or authority.\r\n=======================================================================\r\nCreative Commons is not a party to its public licenses.\r\nNotwithstanding, Creative Commons may elect to apply one of its public\r\nlicenses to material it publishes and in those instances will be\r\nconsidered the “Licensor.” The text of the Creative Commons public\r\nlicenses is dedicated to the public domain under the CC0 Public Domain\r\nDedication. Except for the limited purpose of indicating that material\r\nis shared under a Creative Commons public license or as otherwise\r\npermitted by the Creative Commons policies published at\r\ncreativecommons.org/policies, Creative Commons does not authorize the\r\nuse of the trademark “Creative Commons” or any other trademark or logo\r\nof Creative Commons without its prior written consent including, without\r\nlimitation, in connection with any unauthorized modifications to any of\r\nits public licenses or any other arrangements, understandings, or\r\nagreements concerning use of licensed material. For the avoidance of\r\ndoubt, this paragraph does not form part of the public licenses.\r\nCreative Commons may be contacted at creativecommons.org.\r\n\r\n\r\n",
      "last_modified": "2022-07-06T19:21:36-07:00"
    },
    {
      "path": "schedule.html",
      "title": "Schedule",
      "author": [],
      "contents": "\r\n\r\nContents\r\nWeek 1: Introduction\r\nWeek 2: Data\r\nPreprocessing\r\n\r\n\r\n\r\n\r\nWeek 1: Introduction\r\n\r\n\r\nDate\r\n\r\n\r\nNotes\r\n\r\n\r\nKaggle Notebooks\r\n\r\n\r\nSlides\r\n\r\n\r\nOptional Reading\r\n\r\n\r\nAssignments\r\n\r\n\r\n3-Oct\r\n\r\n\r\nLecture-1\r\n\r\n\r\nNotebook-1\r\n\r\n\r\nWeek\r\n1\r\n\r\n\r\nKaggle\r\nCommonLit Readability\r\nCompetitionNIJ\r\nRecidivism Challenge\r\n\r\n\r\n\r\n\r\nWeek 2: Data Preprocessing\r\n\r\n\r\n\r\n\r\nDate\r\n\r\n\r\nNotes\r\n\r\n\r\nKaggle Notebooks\r\n\r\n\r\nSlides\r\n\r\n\r\nOptional Reading\r\n\r\n\r\nAssignments\r\n\r\n\r\n2\r\n\r\n\r\n10-Oct\r\n\r\n\r\nLecture-2aLecture-2b\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-07-06T19:23:03-07:00"
    },
    {
      "path": "syllabus.html",
      "title": "Course Syllabus",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\r\nCourse syllabus\r\n\r\n\r\n\r\n",
      "last_modified": "2022-07-06T19:21:40-07:00"
    }
  ],
  "collections": []
}
