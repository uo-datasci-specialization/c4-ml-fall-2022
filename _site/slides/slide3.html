<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>An Overview of Linear Regression and Bias-Variance Tradeoff in Predictive Modeling</title>
    <meta charset="utf-8" />
    <meta name="author" content="Cengiz Zopluoglu" />
    <script src="slide3_files/header-attrs-2.14/header-attrs.js"></script>
    <link href="slide3_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="slide3_files/remark-css-0.0.1/uo.css" rel="stylesheet" />
    <link href="slide3_files/remark-css-0.0.1/ki-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my_custom.css" type="text/css" />
    <link rel="stylesheet" href="xaringanthemer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# An Overview of Linear Regression and Bias-Variance Tradeoff in Predictive Modeling
]
.author[
### Cengiz Zopluoglu
]
.institute[
### College of Education, University of Oregon
]
.date[
### Oct 17, 2022 <br> Eugene, OR
]

---


&lt;style&gt;

.blockquote {
  border-left: 5px solid #007935;
  background: #f9f9f9;
  padding: 10px;
  padding-left: 30px;
  margin-left: 16px;
  margin-right: 0;
  border-radius: 0px 4px 4px 0px;
}

#infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid black;
  border-radius: 10px;
  background: #E6F6DC 5px center/3em no-repeat;
}

.centering[
  float: center;
]

.left-column2 {
  width: 50%;
  height: 92%;
  float: left;
  padding-top: 1em;
}

.right-column2 {
  width: 50%;
  float: right;
  padding-top: 1em;
}

.remark-code {
  font-size: 18px;
}

.tiny .remark-code { /*Change made here*/
  font-size: 60% !important;
}

.tiny2 .remark-code { /*Change made here*/
  font-size: 50% !important;
}

.indent {
  margin-left: 3em;
}

.single {
  line-height: 1 ;
}


.double {
  line-height: 2 ;
}

.title-slide h1 {
  padding-top: 0px;
  font-size: 40px;
  text-align: center;
  padding-bottom: 18px;
  margin-bottom: 18px;
}

.title-slide h2 {
  font-size: 30px;
  text-align: center;
  padding-top: 0px;
  margin-top: 0px;
}

.title-slide h3 {
  font-size: 30px;
  color: #26272A;
  text-align: center;
  text-shadow: none;
  padding: 10px;
  margin: 10px;
  line-height: 1.2;
}

&lt;/style&gt;



### Today's Goals:

- An Overview of Linear Regression
  
  - Model Description
  
  - Model Estimation
  
  - Performance Evaluation

- Understanding the concept of bias - variance tradeoff for predictive models

- How to balance the model bias and variance when building predictive models

---

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;center&gt;

# An Overview of Linear Regression
---

- The prediction algorithms are classified into two main categories: *supervised* and *unsupervised*. 

- **Supervised algorithms** are used when the dataset has an actual outcome of interest to predict (labels), and the goal is to build the "best" model predicting the outcome of interest. 

- **Unsupervised algorithms** are used when the dataset doesn't have an outcome of interest. The goal is typically to identify similar groups of observations (rows of data) or similar groups of variables (columns of data) in data. 

- This course will cover several *supervised* algorithms and Linear Regression is one of the most straightforward supervised algorithms and the easiest to interpret.

---


## Model Description

The linear regression model with `\(P\)` predictors and an outcome variable `\(Y\)` can be written as

`$$Y = \beta_0  + \sum_{p=1}^{P} \beta_pX_{p} + \epsilon$$`

In this model, 
  
  - `\(Y\)` represents the observed value for the outcome of an observation, 
  
  - `\(X_{p}\)` represents the observed value of the `\(p^{th}\)` variable for the same observation, 
  
  - `\(\beta_p\)` is the associated model parameter for the `\(p^{th}\)` variable,
  
  - and `\(\epsilon\)` is the model error (residual) for the observation.

This model includes only the main effects of each predictor.

---

- The previous model can be easily extended by including quadratic or higher-order polynomial terms for all (or a specific subset of) predictors. 

- A model with the first-order, second-order, and third-order polynomial terms for all predictors can be written as 

`$$Y = \beta_0  + \sum_{p=1}^{P} \beta_pX_{p} + \sum_{k=1}^{P} \beta_{k+P}X_{k}^2 + \sum_{m=1}^{P} \beta_{m+2P}X_{m}^3 + \epsilon$$`
- Example: A model with only main effects

`$$Y = \beta_0  + \beta_1X_{1} + \beta_2X_{2} + \beta_3X_{3}+ \epsilon.$$`

- Example: A model with polynomial terms up to the 3rd degree added:

`$$Y = \beta_0  + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \\ \beta_4X_1^2 + \beta_5X_2^2 + \beta_6X_3^2+ \\ \beta_{7}X_1^3 + \beta_{8}X_2^3 + \beta_{9}X_3^3$$`


---

- The effect of predictor variables on the outcome variable is sometimes not additive. 

- When the effect of one predictor on the response variable depends on the levels of another predictor,  non-additive effects (a.k.a. interaction effects) can also be added to the model. 

- The interaction effects can be first-order interactions (interaction between two variables, e.g., `\(X_1*X_2\)`), second-order interactions ($X_1*X_2*X_3$), or higher orders.  I

- For instance, the model below also adds the first-order interactions.

`$$Y = \beta_0  + \sum_{p=1}^{P} \beta_pX_{p} + \sum_{k=1}^{P} \beta_{k+P}X_{k}^2 + \sum_{m=1}^{P} \beta_{m+2P}X_{m}^3 + \sum_{i=1}^{P}\sum_{j=i+1}^{P}\beta_{i,j}X_iX_j + \epsilon$$`
- A model with both interaction terms and polynomial terms up to the 3rd degree added:

`$$Y = \beta_0  + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \\ \beta_4X_1^2 + \beta_5X_2^2 + \beta_6X_3^2+ \\ \beta_{7}X_1^3 + \beta_{8}X_2^3 + \beta_{9}X_3^3+ \\ \beta_{1,2}X_1X_2+ \beta_{1,3}X_1X_3 + \beta_{2,3}X_2X_3 + \epsilon$$`

---

## Model Estimation

- Suppose that we would like to predict the target readability score for a given text from the Feature 220.

- Note that there are 768 features extracted from the NLP model as numerical embeddings. For the sake of simplicity, we will only use one of them (Feature 220).

- Below is a scatterplot to show the relationship between these two variables for a random sample of 20 observations. 



&lt;img src="slide3_files/figure-html/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;

---

- Consider a simple linear regression model

  - Outcome: the readability score is the outcome ( `\(Y\)` )
  
  - Predictor: Feature 220 ( `\(X\)` ) 
  
- Our regression model is

`$$Y = \beta_0  + \beta_1X + \epsilon$$`

- The set of coefficients, { `\(\beta_0,\beta_1\)` } , represents a linear line. 

- We can write any set of { `\(\beta_0,\beta_1\)` } coefficients and use it as our model. 

- For instance, suppose we guesstimate that these coefficients are { `\(\beta_0,\beta_1\)` } = {-1.5,2}. Then, my model would be

`$$Y = -1.5  + 2X + \epsilon$$`
---

&lt;img src="slide3_files/figure-html/unnamed-chunk-3-1.svg" style="display: block; margin: auto;" /&gt;

---

We can predict the target readability score for any observation in the dataset using this model. 

`$$Y_{(1)} = -1.5  + 2X_{(1)} + \epsilon_{(1)}.$$`

`$$\hat{Y}_{(1)} =  -1.5 + 2*(-0.139) = -1.778$$`

$$\hat{\epsilon}_{(1)} = -2.062 - (-1.778) =  -0.284 $$

The discrepancy between the observed value and the model prediction is the model error (residual) for the first observation and captured in the `\(\epsilon_{(1)}\)` term in the model.

&lt;img src="slide3_files/figure-html/unnamed-chunk-4-1.svg" style="display: block; margin: auto;" /&gt;

---

We can do the same thing for the second observation.

`$$Y_{(2)} = -1.5  + 2X_{(2)} + \epsilon_{(2)}.$$`

`$$\hat{Y}_{(2)} =  -1.5 + 2*(0.218) = -1.065$$`

$$\hat{\epsilon}_{(2)} = 0.583 - (-1.065) =  1.648 $$

&lt;img src="slide3_files/figure-html/unnamed-chunk-5-1.svg" style="display: block; margin: auto;" /&gt;

---

Using a similar approach, we can calculate the model error for every observation.



&lt;img src="slide3_files/figure-html/unnamed-chunk-7-1.svg" style="display: block; margin: auto;" /&gt;

---

.single[


```r
d &lt;-  readability_sub[,c('V220','target')]

d$predicted &lt;- -1.5 + 2*d$V220
d$error     &lt;- d$target - d$predicted
round(d,3)
```
]
.pull-left[

.single[


```
     V220 target predicted  error
1  -0.139 -2.063    -1.778 -0.285
2   0.218  0.583    -1.065  1.647
3   0.058 -1.653    -1.384 -0.269
4   0.025 -0.874    -1.449  0.576
5   0.224 -1.740    -1.051 -0.689
6  -0.078 -3.640    -1.656 -1.984
7   0.434 -0.623    -0.632  0.009
8  -0.244 -0.344    -1.987  1.643
9   0.159 -1.123    -1.182  0.059
10  0.145 -0.999    -1.210  0.211
11  0.342 -0.877    -0.816 -0.061
12  0.252 -0.033    -0.996  0.963
13  0.035 -0.495    -1.429  0.934
14  0.364  0.125    -0.772  0.896
15  0.300  0.097    -0.900  0.997
16  0.198  0.384    -1.103  1.487
17  0.078 -0.581    -1.344  0.762
18  0.079 -0.343    -1.341  0.998
19  0.570 -0.391    -0.360 -0.031
20  0.345 -0.675    -0.810  0.134
```
]

]

.pull-right[


`$$SSR = \sum_{i=1}^{N}(Y_{(i)} - (\beta_0+\beta_1X_{(i)}))^2$$`

`$$SSR = \sum_{i=1}^{N}(Y_{(i)} - \hat{Y}_{(i)})^2$$`

`$$SSR = \sum_{i=1}^{N}(\epsilon_{(i)})^2$$`

$$ SSR = 17.767$$

For the set of coefficients { `\(\beta_0,\beta_1\)` } = {-1.5,2}, SSR is equal to 17.767.

Could you find another set of coefficients that can do a better job in terms of prediction (smaller SSR)?
]

---

**Thought Experiment**

- Suppose the potential range for my intercept, `\(\beta_0\)`, is from -10 to 10, and we will consider every single possible value from -10 to 10 with increments of .1. 

- Also, suppose the potential range for my slope, `\(\beta_1\)`, is from -5 to 5, and we will consider every single possible value from -5 to 5 with increments of .01. 

- Note that every single possible combination of `\(\beta_0\)` and `\(\beta_1\)` indicates a different model

- How many possible set of coefficients are there? 

- Can we try every single possible set of coefficients  and compute the SSR?

---


```r
grid &lt;- read.csv(here('data/grid_regression.csv'),header=TRUE)

plot_ly(grid, x = ~b0, y = ~b1, z = ~SSR, 
        marker = list(color = ~SSR,
                      showscale = TRUE,
                      cmin=min(grid$SSR)*5,
                      cmax=min(grid$SSR),cauto=F),
        width=600,height=600) %&gt;% 
  add_markers()
```

---

**Optimization**

- In our simple demonstration, estimating the best set of coefficients is an optimization problem with the following loss function:

`$$Loss = \sum_{i=1}^{N}(Y_{(i)} - (\beta_0+\beta_1X_{(i)}))^2$$`

- For a standard linear regression problem, a typical loss function is the **sum of squared residuals**, and we try to pick the set of coefficients that minimize this quantity.

- Numerical approximations are used in most cases for optimization problems.

- In the case of standard linear regression, we can mathematically obtain the complete solution without any numerical approximation.

---

### Matrix Solution

We can find the best set of coefficients for most regression problems with a simple matrix operation. 

First, let's rewrite the regression problem in matrix form. 

&lt;br&gt;

`$$Y_{(1)} = \beta_0  + \beta_1X_{(1)} + \epsilon_{(1)}$$`

`$$Y_{(2)} = \beta_0  + \beta_1X_{(2)} + \epsilon_{(2)}$$`

`$$Y_{(3)} = \beta_0  + \beta_1X_{(3)} + \epsilon_{(3)}$$`
`$$Y_{(4)} = \beta_0  + \beta_1X_{(4)} + \epsilon_{(4)}$$`

`$$Y_{(5)} = \beta_0  + \beta_1X_{(5)} + \epsilon_{(5)}$$`

`$$Y_{(6)} = \beta_0  + \beta_1X_{(6)} + \epsilon_{(6)}$$`

`$$...$$`
`$$...$$`
`$$...$$`
`$$Y_{(20)} = \beta_0  + \beta_1X_{(20)} + \epsilon_{(20)}$$`
---

We can write all of these equations in a much simpler format as

$$ \mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}, $$

- `\(\mathbf{Y}\)` is an N x 1 column vector of observed values for the outcome variable, 

- `\(\mathbf{X}\)` is an N x (P+1) **design matrix** for the set of predictor variables, including an intercept term, 

- `\(\boldsymbol{\beta}\)` is a (P+1) x 1 column vector of regression coefficients, 

- and  `\(\boldsymbol{\epsilon}\)` is an N x 1 column vector of residuals. 

---

These matrix elements would look like the following for the problem above with our small dataset.

.pull-left[

![](regression_matrix.png)

]

.pull-right[

![](data_regression_matrix.gif)
]

---

It can be shown that the set of `\(\boldsymbol{\beta}\)` coefficients that yields the minimum sum of squared residuals for this model can be analytically found using the following matrix operation.

`$$\hat{\boldsymbol{\beta}} = (\mathbf{X^T}\mathbf{X})^{-1}\mathbf{X^T}\mathbf{Y}$$`

Suppose we apply this matrix operation for the previous example.

.single[

```r
Y &lt;-  as.matrix(readability_sub$target)
X &lt;-  as.matrix(cbind(1,readability_sub$V220))

beta &lt;- solve(t(X)%*%X)%*%t(X)%*%Y

beta 
```

```
          [,1]
[1,] -1.108295
[2,]  2.048931
```
]

The best set of  { `\(\beta_0,\beta_1\)` } coefficients to predict the readability score with the least amount of error using Feature 220 as a predictor is { `\(\beta_0,\beta_1\)` } = {-1.108, 2.049}

These estimates are also known as the **least square estimates**, and the best linear unbiased estimators (BLUE) for the given regression model.

---

Once we find the best estimates for the model coefficients, we can also calculate the model predicted values and residual sum of squares for the given model and dataset.

$$\boldsymbol{\hat{Y}} = \mathbf{X} \hat{\boldsymbol{\beta}} $$

$$ \boldsymbol{\hat{\epsilon}} = \boldsymbol{Y} - \hat{\boldsymbol{Y}} $$

$$ SSR = \boldsymbol{\hat{\epsilon}^T} \boldsymbol{\hat{\epsilon}} $$

.single[

```r
Y_hat &lt;-  X%*%beta

E &lt;- Y - Y_hat

SSR &lt;- t(E)%*%E

SSR
```

```
         [,1]
[1,] 14.56567
```
]

---

- The matrix formulation is generalized to a regression model for more than one predictor. 

- When there are more predictors in the model, the dimensions of the design matrix, `\(\mathbf{X}\)`, and regression coefficient matrix, `\(\boldsymbol{\beta}\)`, will be different, but the matrix calculations will be identical. 

- Assume that we would like to expand our model by adding another predictor, Feature 166 as the second predictor. Our new model will be

`$$Y_{(i)} = \beta_0  + \beta_1X_{1(i)} + \beta_2X_{2(i)} + \epsilon_{(i)}$$`

- `\(X_1\)` represents Feature 220 and `\(X_2\)` represents Feature 166. 

- Now, we are looking for the best set of three coefficients, { `\(\beta_0, \beta_1, \beta_2\)` } that would yield the least error in predicting the readability.

---

.single[

```r
Y &lt;-  as.matrix(readability_sub$target)
X &lt;-  as.matrix(cbind(1,readability_sub[,c('V220','V166')]))
```
]

.pull-left[

.single[

```r
X
```

```
      1        V220        V166
 [1,] 1 -0.13908258  0.19028091
 [2,] 1  0.21764143  0.07101288
 [3,] 1  0.05812133  0.03993277
 [4,] 1  0.02526429  0.18845809
 [5,] 1  0.22430885  0.06200715
 [6,] 1 -0.07795373  0.10754109
 [7,] 1  0.43400714  0.12202360
 [8,] 1 -0.24364550  0.02454670
 [9,] 1  0.15893717  0.10422343
[10,] 1  0.14496475  0.02339597
[11,] 1  0.34222975  0.22065343
[12,] 1  0.25219145  0.10865010
[13,] 1  0.03532625  0.07549474
[14,] 1  0.36410633  0.18675801
[15,] 1  0.29988593  0.11618323
[16,] 1  0.19837037  0.08272671
[17,] 1  0.07807041  0.10235218
[18,] 1  0.07935690  0.11618605
[19,] 1  0.57000953 -0.02385423
[20,] 1  0.34523284  0.09299514
```
]
]

.pull-right[

.single[

```r
Y
```

```
             [,1]
 [1,] -2.06282395
 [2,]  0.58258607
 [3,] -1.65313060
 [4,] -0.87390681
 [5,] -1.74049148
 [6,] -3.63993555
 [7,] -0.62284268
 [8,] -0.34426981
 [9,] -1.12298826
[10,] -0.99857142
[11,] -0.87656742
[12,] -0.03304643
[13,] -0.49529863
[14,]  0.12453660
[15,]  0.09678258
[16,]  0.38422270
[17,] -0.58143038
[18,] -0.34324576
[19,] -0.39054205
[20,] -0.67548411
```
]

]

---

.single[


```r
beta &lt;- solve(t(X)%*%X)%*%t(X)%*%Y

beta 
```

```
           [,1]
1    -1.0068344
V220  2.0363683
V166 -0.9877414
```

&lt;br&gt;


```r
Y_hat &lt;-  X%*%beta

E &lt;- Y - Y_hat

SSR &lt;- t(E)%*%E

SSR
```

```
         [,1]
[1,] 14.49461
```
]


---

### `lm()` function

- While learning the inner mechanics of how numbers work behind the scenes is always exciting, it is handy to use already existing packages and tools to do all these computations. 

- A simple go-to function for fitting a linear regression to predict a continuous outcome is the `lm()` function.

**Model 1: Predicting readability scores from Feature 220**

.single[.tiny[

```r
mod &lt;- lm(target ~ 1 + V220,data=readability_sub)

summary(mod)
```

```

Call:
lm(formula = target ~ 1 + V220, data = readability_sub)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.37192 -0.45499 -0.00234  0.56655  1.26324 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -1.1083     0.2662  -4.163 0.000584 ***
V220          2.0489     1.0356   1.978 0.063390 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.8996 on 18 degrees of freedom
Multiple R-squared:  0.1786,	Adjusted R-squared:  0.133 
F-statistic: 3.914 on 1 and 18 DF,  p-value: 0.06339
```
]]

---

**Model 2: Predicting readability scores from Feature 220 and Feature 166**


.single[.tiny[

```r
mod &lt;- lm(target ~ 1 + V220 + V166,data=readability_sub)

summary(mod)
```

```

Call:
lm(formula = target ~ 1 + V220 + V166, data = readability_sub)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.3681 -0.4265  0.0019  0.5827  1.2164 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  -1.0068     0.4452  -2.262   0.0371 *
V220          2.0364     1.0639   1.914   0.0726 .
V166         -0.9877     3.4214  -0.289   0.7763  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9234 on 17 degrees of freedom
Multiple R-squared:  0.1826,	Adjusted R-squared:  0.08646 
F-statistic: 1.899 on 2 and 17 DF,  p-value: 0.1801
```
]]

---

## Performance Evaluation

### Accuracy Metrics

- **Mean Absolute Error(MAE)**

$$ MAE = \frac{\sum_{i=1}^{N} \left | e_i \right |}{N}$$

- **Mean Squared Error(MSE)**

$$ MSE = \frac{\sum_{i=1}^{N} e_i^{2}}{N}$$

- **Root Mean Squared Error (RMSE)**

$$ RMSE = \sqrt{\frac{\sum_{i=1}^{N} e_i^{2}}{N}}$$

---

If we take predictions from the second model, we can calculate these performance metrics for the small demo dataset using the following code.

.single[.tiny2[

```r
mod &lt;- lm(target ~ 1 + V220 + V166,data=readability_sub)

readability_sub$pred &lt;- predict(mod)

readability_sub[,c('target','pred')]
```

```
        target       pred
1  -2.06282395 -1.4780061
2   0.58258607 -0.6337787
3  -1.65313060 -0.9279212
4  -0.87390681 -1.1415349
5  -1.74049148 -0.6113060
6  -3.63993555 -1.2717997
7  -0.62284268 -0.2435638
8  -0.34426981 -1.5272322
9  -1.12298826 -0.7861256
10 -0.99857142 -0.7347420
11 -0.87656742 -0.5278772
12 -0.03304643 -0.6005980
13 -0.49529863 -1.0094665
14  0.12453660 -0.4498485
15  0.09678258 -0.5109152
16  0.38422270 -0.6845919
17 -0.58143038 -0.9489518
18 -0.34324576 -0.9599963
19 -0.39054205  0.1774767
20 -0.67548411 -0.3956684
```

```r
# Mean absolute error

  mean(abs(readability_sub$target - readability_sub$pred))
```

```
[1] 0.6983844
```

```r
# Mean squared error

  mean((readability_sub$target - readability_sub$pred)^2)
```

```
[1] 0.7247307
```

```r
# Root mean squared error

  sqrt(mean((readability_sub$target - readability_sub$pred)^2))
```

```
[1] 0.8513112
```
]]

---

### Proportional Reduction in Total Amount of Error (R-squared)

`\(SSR_{null}\)` : sum of squared residuals when we use only mean to predict the outcome (intercept-only model)

`$$SSR_{null} = \sum_{i=1}^{N} (y-\bar{y})^2$$`

In our case, if we use the mean to predict the outcome for each observation, the sum of squared error would be equal to 17.733.

.single[

```r
y_bar &lt;- mean(readability_sub$target)

ssr_null &lt;- sum((readability_sub$target-y_bar)^2)

ssr_null
```

```
[1] 17.73309
```
]

---

Instead, if we rely on our model (Feature 220 + Feature 166) to predict the outcome, the sum of squared error would be equal to 14.495.


```r
ssr_model &lt;- sum((readability_sub$target - readability_sub$pred)^2)

ssr_model
```

```
[1] 14.49461
```

The total amount of prediction error is reduced by about 18.3% when we use our model instead of a simple null model. This can be used as a performance measure for any model.

`$$1-\frac{SSR_{model}}{SSR_{null}}$$`


```r
1 - (ssr_model/ssr_null)
```

```
[1] 0.1826235
```


---

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;center&gt;

# Bias - Variance Tradeoff for Predictive Models
---




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
